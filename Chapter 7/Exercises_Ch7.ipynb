{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise: 7-2\n",
    "# Write a tag pattern to match noun phrases containing plural head nouns, e.g., many/JJ researchers/NNS, two/CD\n",
    "# weeks/NNS, both/DT new/JJ positions/NNS. Try to do this by generalizing the tag pattern that handled singular\n",
    "# noun phrases.\n",
    "\n",
    "import nltk\n",
    "\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<CD|JJ><NN.>}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP many/JJ researchers/NNS))\n",
      "(S (NP two/CD weeks/NNS))\n",
      "(S (NP both/DT new/JJ positions/NNS))\n"
     ]
    }
   ],
   "source": [
    "nps =[[(\"many\", \"JJ\"), (\"researchers\", \"NNS\")],\n",
    "      [(\"two\", \"CD\"), (\"weeks\", \"NNS\")],\n",
    "      [(\"both\", \"DT\"), (\"new\", \"JJ\"), (\"positions\", \"NNS\")]]\n",
    "\n",
    "for n in nps:\n",
    "    print(cp.parse(n))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (S\n",
      "  Confidence/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  another/DT\n",
      "  sharp/JJ\n",
      "  dive/NN\n",
      "  if/IN\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  for/IN\n",
      "  September/NNP\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  for/IN\n",
      "  release/NN\n",
      "  tomorrow/NN\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  a/DT\n",
      "  substantial/JJ\n",
      "  improvement/NN\n",
      "  from/IN\n",
      "  July/NNP\n",
      "  and/CC\n",
      "  August/NNP\n",
      "  's/POS\n",
      "  near-record/JJ\n",
      "  deficits/NNS\n",
      "  ./.)\n",
      "1 (S\n",
      "  Chancellor/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  Exchequer/NNP\n",
      "  Nigel/NNP\n",
      "  Lawson/NNP\n",
      "  's/POS\n",
      "  restated/VBN\n",
      "  commitment/NN\n",
      "  to/TO\n",
      "  a/DT\n",
      "  firm/NN\n",
      "  monetary/JJ\n",
      "  policy/NN\n",
      "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
      "  a/DT\n",
      "  freefall/NN\n",
      "  in/IN\n",
      "  sterling/NN\n",
      "  over/IN\n",
      "  the/DT\n",
      "  past/JJ\n",
      "  week/NN\n",
      "  ./.)\n",
      "2 (S\n",
      "  But/CC\n",
      "  analysts/NNS\n",
      "  (VP reckon/VBP)\n",
      "  underlying/VBG\n",
      "  support/NN\n",
      "  for/IN\n",
      "  sterling/NN\n",
      "  (VP has/VBZ been/VBN eroded/VBN)\n",
      "  by/IN\n",
      "  the/DT\n",
      "  chancellor/NN\n",
      "  's/POS\n",
      "  failure/NN\n",
      "  (VP to/TO announce/VB)\n",
      "  any/DT\n",
      "  new/JJ\n",
      "  policy/NN\n",
      "  measures/NNS\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  Mansion/NNP\n",
      "  House/NNP\n",
      "  speech/NN\n",
      "  last/JJ\n",
      "  Thursday/NNP\n",
      "  ./.)\n",
      "3 (S\n",
      "  This/DT\n",
      "  (VP has/VBZ increased/VBN)\n",
      "  the/DT\n",
      "  risk/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  government/NN\n",
      "  (VP being/VBG forced/VBN to/TO increase/VB)\n",
      "  base/NN\n",
      "  rates/NNS\n",
      "  to/TO\n",
      "  16/CD\n",
      "  %/NN\n",
      "  from/IN\n",
      "  their/PRP$\n",
      "  current/JJ\n",
      "  15/CD\n",
      "  %/NN\n",
      "  level/NN\n",
      "  (VP to/TO defend/VB)\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  ,/,\n",
      "  economists/NNS\n",
      "  and/CC\n",
      "  foreign/JJ\n",
      "  exchange/NN\n",
      "  market/NN\n",
      "  analysts/NNS\n",
      "  (VP say/VBP)\n",
      "  ./.)\n",
      "4 (S\n",
      "  ``/``\n",
      "  The/DT\n",
      "  risks/NNS\n",
      "  for/IN\n",
      "  sterling/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  bad/JJ\n",
      "  trade/NN\n",
      "  figure/NN\n",
      "  (VP are/VBP)\n",
      "  very/RB\n",
      "  heavily/RB\n",
      "  on/IN\n",
      "  the/DT\n",
      "  down/JJ\n",
      "  side/NN\n",
      "  ,/,\n",
      "  ''/''\n",
      "  (VP said/VBD)\n",
      "  Chris/NNP\n",
      "  Dillow/NNP\n",
      "  ,/,\n",
      "  senior/JJ\n",
      "  U.K./NNP\n",
      "  economist/NN\n",
      "  at/IN\n",
      "  Nomura/NNP\n",
      "  Research/NNP\n",
      "  Institute/NNP\n",
      "  ./.)\n",
      "5 (S\n",
      "  ``/``\n",
      "  If/IN\n",
      "  there/EX\n",
      "  (VP is/VBZ)\n",
      "  another/DT\n",
      "  bad/JJ\n",
      "  trade/NN\n",
      "  number/NN\n",
      "  ,/,\n",
      "  there/EX\n",
      "  (VP could/MD be/VB)\n",
      "  an/DT\n",
      "  awful/JJ\n",
      "  lot/NN\n",
      "  of/IN\n",
      "  pressure/NN\n",
      "  ,/,\n",
      "  ''/''\n",
      "  (VP noted/VBD)\n",
      "  Simon/NNP\n",
      "  Briscoe/NNP\n",
      "  ,/,\n",
      "  U.K./NNP\n",
      "  economist/NN\n",
      "  for/IN\n",
      "  Midland/NNP\n",
      "  Montagu/NNP\n",
      "  ,/,\n",
      "  a/DT\n",
      "  unit/NN\n",
      "  of/IN\n",
      "  Midland/NNP\n",
      "  Bank/NNP\n",
      "  PLC/NNP\n",
      "  ./.)\n",
      "6 (S\n",
      "  Forecasts/NNS\n",
      "  for/IN\n",
      "  the/DT\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  (VP range/VBP)\n",
      "  widely/RB\n",
      "  ,/,\n",
      "  but/CC\n",
      "  few/JJ\n",
      "  economists/NNS\n",
      "  (VP expect/VBP)\n",
      "  the/DT\n",
      "  data/NNS\n",
      "  (VP to/TO show/VB)\n",
      "  a/DT\n",
      "  very/RB\n",
      "  marked/VBN\n",
      "  improvement/NN\n",
      "  from/IN\n",
      "  the/DT\n",
      "  #/#\n",
      "  2/CD\n",
      "  billion/CD\n",
      "  -LRB-/(\n",
      "  $/$\n",
      "  3.2/CD\n",
      "  billion/CD\n",
      "  -RRB-/)\n",
      "  deficit/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  current/JJ\n",
      "  account/NN\n",
      "  (VP reported/VBD)\n",
      "  for/IN\n",
      "  August/NNP\n",
      "  ./.)\n",
      "7 (S\n",
      "  The/DT\n",
      "  August/NNP\n",
      "  deficit/NN\n",
      "  and/CC\n",
      "  the/DT\n",
      "  #/#\n",
      "  2.2/CD\n",
      "  billion/CD\n",
      "  gap/NN\n",
      "  (VP registered/VBN)\n",
      "  in/IN\n",
      "  July/NNP\n",
      "  (VP are/VBP topped/VBN)\n",
      "  only/RB\n",
      "  by/IN\n",
      "  the/DT\n",
      "  #/#\n",
      "  2.3/CD\n",
      "  billion/CD\n",
      "  deficit/NN\n",
      "  of/IN\n",
      "  October/NNP\n",
      "  1988/CD\n",
      "  ./.)\n",
      "8 (S\n",
      "  Sanjay/NNP\n",
      "  Joshi/NNP\n",
      "  ,/,\n",
      "  European/JJ\n",
      "  economist/NN\n",
      "  at/IN\n",
      "  Baring/NNP\n",
      "  Brothers/NNPS\n",
      "  &/CC\n",
      "  Co./NNP\n",
      "  ,/,\n",
      "  (VP said/VBD)\n",
      "  there/EX\n",
      "  (VP is/VBZ)\n",
      "  no/DT\n",
      "  sign/NN\n",
      "  that/IN\n",
      "  Britain/NNP\n",
      "  's/POS\n",
      "  manufacturing/NN\n",
      "  industry/NN\n",
      "  (VP is/VBZ transforming/VBG)\n",
      "  itself/PRP\n",
      "  (VP to/TO boost/VB)\n",
      "  exports/NNS\n",
      "  ./.)\n",
      "9 (S\n",
      "  At/IN\n",
      "  the/DT\n",
      "  same/JJ\n",
      "  time/NN\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  (VP remains/VBZ)\n",
      "  fairly/RB\n",
      "  pessimistic/JJ\n",
      "  about/IN\n",
      "  the/DT\n",
      "  outlook/NN\n",
      "  for/IN\n",
      "  imports/NNS\n",
      "  ,/,\n",
      "  given/VBN\n",
      "  continued/VBD\n",
      "  high/JJ\n",
      "  consumer/NN\n",
      "  and/CC\n",
      "  capital/NN\n",
      "  goods/NNS\n",
      "  inflows/NNS\n",
      "  ./.)\n",
      "10 (S\n",
      "  He/PRP\n",
      "  (VP reckons/VBZ)\n",
      "  the/DT\n",
      "  current/JJ\n",
      "  account/NN\n",
      "  deficit/NN\n",
      "  (VP will/MD narrow/VB)\n",
      "  to/TO\n",
      "  only/RB\n",
      "  #/#\n",
      "  1.8/CD\n",
      "  billion/CD\n",
      "  in/IN\n",
      "  September/NNP\n",
      "  ./.)\n",
      "11 (S\n",
      "  However/RB\n",
      "  ,/,\n",
      "  Mr./NNP\n",
      "  Dillow/NNP\n",
      "  (VP said/VBD)\n",
      "  he/PRP\n",
      "  (VP believes/VBZ)\n",
      "  that/IN\n",
      "  a/DT\n",
      "  reduction/NN\n",
      "  in/IN\n",
      "  raw/JJ\n",
      "  material/NN\n",
      "  stockbuilding/VBG\n",
      "  by/IN\n",
      "  industry/NN\n",
      "  (VP could/MD lead/VB)\n",
      "  to/TO\n",
      "  a/DT\n",
      "  sharp/JJ\n",
      "  drop/NN\n",
      "  in/IN\n",
      "  imports/NNS\n",
      "  ./.)\n",
      "12 (S\n",
      "  Combined/VBN\n",
      "  with/IN\n",
      "  at/IN\n",
      "  least/JJS\n",
      "  some/DT\n",
      "  rebound/NN\n",
      "  in/IN\n",
      "  exports/NNS\n",
      "  after/IN\n",
      "  August/NNP\n",
      "  's/POS\n",
      "  unexpected/JJ\n",
      "  decline/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  deficit/NN\n",
      "  (VP could/MD narrow/VB)\n",
      "  to/TO\n",
      "  as/RB\n",
      "  little/JJ\n",
      "  as/IN\n",
      "  #/#\n",
      "  1.3/CD\n",
      "  billion/CD\n",
      "  ./.)\n",
      "13 (S\n",
      "  Mr./NNP\n",
      "  Briscoe/NNP\n",
      "  ,/,\n",
      "  who/WP\n",
      "  also/RB\n",
      "  (VP forecasts/VBZ)\n",
      "  a/DT\n",
      "  #/#\n",
      "  1.3/CD\n",
      "  billion/CD\n",
      "  current/JJ\n",
      "  account/NN\n",
      "  gap/NN\n",
      "  ,/,\n",
      "  (VP warns/VBZ)\n",
      "  that/IN\n",
      "  even/RB\n",
      "  if/IN\n",
      "  the/DT\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  (VP are/VBP)\n",
      "  bullish/JJ\n",
      "  for/IN\n",
      "  sterling/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  currency/NN\n",
      "  (VP wo/MD n't/RB advance/VB)\n",
      "  much/JJ\n",
      "  because/IN\n",
      "  investors/NNS\n",
      "  (VP will/MD want/VB to/TO see/VB)\n",
      "  further/JJ\n",
      "  evidence/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  turnaround/NN\n",
      "  before/IN\n",
      "  (VP adjusting/VBG)\n",
      "  positions/NNS\n",
      "  ./.)\n",
      "14 (S\n",
      "  Nevertheless/RB\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  (VP noted/VBD)\n",
      "  ,/,\n",
      "  ``/``\n",
      "  No/DT\n",
      "  one/PRP\n",
      "  (VP will/MD want/VB to/TO go/VB)\n",
      "  into/IN\n",
      "  the/DT\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  without/IN\n",
      "  a/DT\n",
      "  flat/JJ\n",
      "  position/NN\n",
      "  ''/''\n",
      "  in/IN\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  ./.)\n",
      "15 (S\n",
      "  Meanwhile/RB\n",
      "  ,/,\n",
      "  overall/JJ\n",
      "  evidence/NN\n",
      "  on/IN\n",
      "  the/DT\n",
      "  economy/NN\n",
      "  (VP remains/VBZ)\n",
      "  fairly/RB\n",
      "  clouded/VBN\n",
      "  ./.)\n",
      "16 (S\n",
      "  In/IN\n",
      "  his/PRP$\n",
      "  Mansion/NNP\n",
      "  House/NNP\n",
      "  speech/NN\n",
      "  ,/,\n",
      "  Mr./NNP\n",
      "  Lawson/NNP\n",
      "  (VP warned/VBD)\n",
      "  that/IN\n",
      "  a/DT\n",
      "  further/JJ\n",
      "  slowdown/NN\n",
      "  (VP can/MD be/VB expected/VBN)\n",
      "  as/IN\n",
      "  the/DT\n",
      "  impact/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  last/JJ\n",
      "  rise/NN\n",
      "  in/IN\n",
      "  interest/NN\n",
      "  rates/NNS\n",
      "  earlier/RBR\n",
      "  this/DT\n",
      "  month/NN\n",
      "  (VP takes/VBZ)\n",
      "  effect/NN\n",
      "  ./.)\n",
      "17 (S\n",
      "  U.K./JJ\n",
      "  base/NN\n",
      "  rates/NNS\n",
      "  (VP are/VBP)\n",
      "  at/IN\n",
      "  their/PRP$\n",
      "  highest/JJS\n",
      "  level/NN\n",
      "  in/IN\n",
      "  eight/CD\n",
      "  years/NNS\n",
      "  ./.)\n",
      "18 (S\n",
      "  But/CC\n",
      "  consumer/NN\n",
      "  expenditure/NN\n",
      "  data/NNS\n",
      "  (VP released/VBD)\n",
      "  Friday/NNP\n",
      "  (VP do/VBP n't/RB suggest/VB)\n",
      "  that/IN\n",
      "  the/DT\n",
      "  U.K./NNP\n",
      "  economy/NN\n",
      "  (VP is/VBZ slowing/VBG)\n",
      "  that/DT\n",
      "  quickly/RB\n",
      "  ./.)\n",
      "19 (S\n",
      "  The/DT\n",
      "  figures/NNS\n",
      "  (VP show/VBP)\n",
      "  that/DT\n",
      "  spending/NN\n",
      "  (VP rose/VBD)\n",
      "  0.1/CD\n",
      "  %/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  third/JJ\n",
      "  quarter/NN\n",
      "  from/IN\n",
      "  the/DT\n",
      "  second/JJ\n",
      "  quarter/NN\n",
      "  and/CC\n",
      "  (VP was/VBD)\n",
      "  up/IN\n",
      "  3.8/CD\n",
      "  %/NN\n",
      "  from/IN\n",
      "  a/DT\n",
      "  year/NN\n",
      "  ago/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 7-3\n",
    "# Pick one of the three chunk types in the CoNLL-2000 Chunking Corpus. Inspect the data and try to observe any\n",
    "# patterns in the POS tag sequences that make up this kind of chunk. Develop a simple chunker using the regular\n",
    "# expression chunker nltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably.\n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "for i in range(20):\n",
    "    print(i, conll2000.chunked_sents('train.txt', chunk_types = ['VP'])[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.3%%\n",
      "    Precision:     64.2%%\n",
      "    Recall:        80.4%%\n",
      "    F-Measure:     71.4%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<[VRMT].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.3%%\n",
      "    Precision:     74.4%%\n",
      "    Recall:        66.8%%\n",
      "    F-Measure:     70.4%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<VB.>?<RB>*<MD>?<VB.>?<TO>?<VB.>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.4%%\n",
      "    Precision:     72.7%%\n",
      "    Recall:        66.9%%\n",
      "    F-Measure:     69.7%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<VB.>?<RB.>*<MD>?<VB.>?<TO>?<VB.>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.3%%\n",
      "    Precision:     74.6%%\n",
      "    Recall:        66.9%%\n",
      "    F-Measure:     70.5%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"VP: {<VB.>?<RB>*<MD>?<VB.>?<TO>?<MD>?<RB>*<VB.>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.3%%\n",
      "    Precision:     60.5%%\n",
      "    Recall:        74.2%%\n",
      "    F-Measure:     66.7%%\n"
     ]
    }
   ],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['VP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types = ['VP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(unigram_chunker.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  96.5%%\n",
      "    Precision:     75.1%%\n",
      "    Recall:        85.7%%\n",
      "    F-Measure:     80.0%%\n"
     ]
    }
   ],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  65.5%%\n",
      "    Precision:     32.6%%\n",
      "    Recall:        26.0%%\n",
      "    F-Measure:     28.9%%\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 7-4\n",
    "# An early definition of chunk was the material that occurs between chinks. Develop a chunker that starts by\n",
    "# putting the whole sentence in a single chunk, and then does the rest of its work solely by chinking. Determine\n",
    "# which tags (or tag sequences) are most likely to make up chinks with the help of your own utility program.\n",
    "# Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules.\n",
    "\n",
    "import nltk, re\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}\n",
    "    }<VB.|IN>+{\n",
    "  \"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[ImmutableTree('NP', [('$', '$'), ('47', 'CD'), (',', ',')]),\n ImmutableTree('NP', [('well', 'RB')]),\n ImmutableTree('NP', [('--', ':'), ('Bruce', 'NNP'), ('Kafaroff', 'NNP'), ('.', '.')]),\n ImmutableTree('NP', [('the', 'DT'), ('magazine', 'NN'), ('.', '.')]),\n ImmutableTree('NP', [('1', 'CD'), ('3\\\\/8', 'CD'), ('to', 'TO'), ('58', 'CD'), ('1\\\\/2', 'CD'), (';', ':'), ('Coca-Cola', 'NNP'), ('Co.', 'NNP')]),\n ImmutableTree('NP', [('a', 'DT'), ('moderate', 'JJ'), ('3.5', 'CD'), ('million', 'CD'), ('ounces', 'NNS'), ('.', '.')]),\n ImmutableTree('NP', [('an', 'DT')]),\n ImmutableTree('NP', [('medical', 'JJ'), ('therapy', 'NN'), ('.', '.')]),\n ImmutableTree('NP', [(',', ',')]),\n ImmutableTree('NP', [('America', 'NNP'), (\"'s\", 'POS'), ('long', 'JJ'), ('history', 'NN')]),\n ImmutableTree('NP', [('investors', 'NNS'), ('to', 'TO'), ('focus', 'VB')]),\n ImmutableTree('NP', [('mostly', 'RB'), ('to', 'TO')]),\n ImmutableTree('NP', [('1936', 'CD'), (',', ','), ('John', 'NNP'), ('Maynard', 'NNP'), ('Keynes', 'NNP')]),\n ImmutableTree('NP', [('long-term', 'JJ'), ('health', 'NN'), ('care', 'NN'), ('will', 'MD'), ('again', 'RB'), ('push', 'VB'), ('lawmakers', 'NNS')]),\n ImmutableTree('NP', [('JUMPING', 'NNP'), ('THE', 'DT'), ('GUN', 'NNP'), (':', ':')]),\n ImmutableTree('NP', [('about', 'RB'), ('$', '$'), ('1', 'CD'), ('trillion', 'CD'), ('a', 'DT'), ('year', 'NN'), ('.', '.')]),\n ImmutableTree('NP', [('fixtures', 'NNS')]),\n ImmutableTree('NP', [('external', 'JJ'), ('pressure', 'NN')]),\n ImmutableTree('NP', [('the', 'DT'), ('courtyard', 'NN'), ('naked', 'JJ'), (',', ',')]),\n ImmutableTree('NP', [('The', 'DT'), ('initial', 'JJ'), ('$', '$'), ('11', 'CD'), ('million', 'CD'), ('research', 'NN'), ('program', 'NN'), ('will', 'MD'), ('conduct', 'VB'), ('the', 'DT'), ('most', 'RBS'), ('extensive', 'JJ'), ('testing', 'NN'), ('to', 'TO'), ('date', 'VB')])]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).incorrect()[20:40]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  85.6%%\n",
      "    Precision:     64.8%%\n",
      "    Recall:        68.0%%\n",
      "    F-Measure:     66.3%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}\n",
    "    }<VB.|IN|,|.|CC|TO|''|MD|``|RB|>+{\n",
    "  \"\"\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP the/DT receiving/VBG end/NN))\n",
      "(S (NP assistant/NN managing/VBG editor/NN))\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 7-5\n",
    "# Write a tag pattern to cover noun phrases that contain gerunds, e.g., the/DT receiving/VBG end/NN, assistant/NN\n",
    "# managing/VBG editor/NN. Add these patterns to the grammar, one per line. Test your work using some tagged\n",
    "# sentences of your own devising.\n",
    "\n",
    "grammar = \"\"\"\n",
    "    NP: {<DT><VBG><NN>}    # chunk determiner, gerund, and noun\n",
    "        {<NN><VBG><NN>}    # chunk noun, gerund, and noun\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentences = [[(\"the\", \"DT\"), (\"receiving\", \"VBG\"), (\"end\", \"NN\")],\n",
    "             [(\"assistant\", \"NN\"),  (\"managing\", \"VBG\"),  (\"editor\", \"NN\")]]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(cp.parse(sent))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP the/DT receiving/VBG end/NN))\n",
      "(S (NP assistant/NN managing/VBG editor/NN))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT|NN><VBG><NN>}    # chunk determiner/noun, gerund, and noun\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentences = [[(\"the\", \"DT\"), (\"receiving\", \"VBG\"), (\"end\", \"NN\")],\n",
    "             [(\"assistant\", \"NN\"),  (\"managing\", \"VBG\"),  (\"editor\", \"NN\")]]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(cp.parse(sent))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP a/DT thriving/VBG metropolis/NN))\n",
      "(S (NP temporary/NN acting/VBG director/NN))\n"
     ]
    }
   ],
   "source": [
    "sentences = [[(\"a\", \"DT\"), (\"thriving\", \"VBG\"), (\"metropolis\", \"NN\")],\n",
    "             [(\"temporary\", \"NN\"),  (\"acting\", \"VBG\"),  (\"director\", \"NN\")]]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(cp.parse(sent))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP July/NNP and/CC August/NNP))\n",
      "(S (NP all/DT your/PRP$ managers/NNS and/CC supervisors/NNS))\n",
      "(S (NP company/NN courts/NNS and/CC adjudicators/NNS))\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 7-6\n",
    "# Write one or more tag patterns to handle coordinated noun phrases, e.g., July/NNP and/CC August/NNP, all/DT\n",
    "# your/PRP$ managers/NNS and/CC supervisors/NNS, company/NN courts/NNS and/CC adjudicators/NNS.\n",
    "\n",
    "grammar = \"\"\"\n",
    "    NP: {<DT>?<PRP.>?<NN.*>+<CC><NN.>} # Chunk coordinated noun phrases\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentences = [[(\"July\", \"NNP\"),  (\"and\", \"CC\"), (\"August\", \"NNP\")],\n",
    "             [(\"all\", \"DT\"), (\"your\", \"PRP$\"), (\"managers\", \"NNS\"),\n",
    "              (\"and\", \"CC\"), (\"supervisors\", \"NNS\")],\n",
    "             [(\"company\", \"NN\"), (\"courts\", \"NNS\"),\n",
    "              (\"and\", \"CC\"), (\"adjudicators\", \"NNS\")]]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(cp.parse(sent))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.3%%\n",
      "    Precision:     74.6%%\n",
      "    Recall:        66.9%%\n",
      "    F-Measure:     70.5%%\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 7-7\n",
    "# Carry out the following evaluation tasks for any of the chunkers you have developed earlier. (Note that most\n",
    "# chunking corpora contain some internal inconsistencies, such that any reasonable rule-based approach will\n",
    "# produce errors.)\n",
    "# a: Evaluate your chunker on 100 sentences from a chunked corpus, and report the precision, recall, and F-measure.\n",
    "\n",
    "grammar = r\"VP: {<VB.>?<RB>*<MD>?<VB.>?<TO>?<MD>?<RB>*<VB.>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt'[:100], chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[ImmutableTree('VP', [(\"'ll\", 'MD'), ('have', 'VB'), ('to', 'TO'), ('see', 'VB')]),\n ImmutableTree('VP', [('to', 'TO'), ('bid', 'VB')]),\n ImmutableTree('VP', [('declined', 'VBD'), ('to', 'TO'), ('comment', 'VB')]),\n ImmutableTree('VP', [('are', 'VBP'), ('experiencing', 'VBG')]),\n ImmutableTree('VP', [('to', 'TO'), ('yield', 'VB')]),\n ImmutableTree('VP', [('to', 'TO'), ('fuel', 'VB')]),\n ImmutableTree('VP', [('says', 'NNS')]),\n ImmutableTree('VP', [('said', 'VBD')]),\n ImmutableTree('VP', [('frequently', 'RB'), ('attempting', 'VBG'), ('to', 'TO'), ('enforce', 'VB')]),\n ImmutableTree('VP', [('would', 'MD'), ('not', 'RB'), ('honor', 'VB')]),\n ImmutableTree('VP', [('to', 'TO'), ('arrest', 'VB')]),\n ImmutableTree('VP', [('anticipate', 'VB'), ('filing', 'NN')]),\n ImmutableTree('VP', [('will', 'MD'), ('continue', 'VB')]),\n ImmutableTree('VP', [('has', 'VBZ'), ('forced', 'VBN')]),\n ImmutableTree('VP', [('show', 'VBP')]),\n ImmutableTree('VP', [('proceed', 'VB')]),\n ImmutableTree('VP', [('will', 'MD'), ('be', 'VB'), ('waived', 'VBN')]),\n ImmutableTree('VP', [('to', 'TO'), ('drum', 'VB')]),\n ImmutableTree('VP', [('would', 'MD'), ('be', 'VB')]),\n ImmutableTree('VP', [('can', 'MD'), ('go', 'VB')])]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b: Use the chunkscore.missed() and chunkscore.incorrect() methods to identify the errors made by your chunker.\n",
    "# Discuss.\n",
    "\n",
    "cp.evaluate(test_sents).missed()[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[ImmutableTree('VP', [('proposed', 'VBN')]),\n ImmutableTree('VP', [('abated', 'VBN')]),\n ImmutableTree('VP', [('managing', 'VBG')]),\n ImmutableTree('VP', [('according', 'VBG')]),\n ImmutableTree('VP', [('stabilizing', 'VBG')]),\n ImmutableTree('VP', [('including', 'VBG')]),\n ImmutableTree('VP', [('based', 'VBN')]),\n ImmutableTree('VP', [('including', 'VBG')]),\n ImmutableTree('VP', [('continued', 'VBD')]),\n ImmutableTree('VP', [('already', 'RB'), ('sells', 'VBZ')]),\n ImmutableTree('VP', [('realized', 'VBN'), ('had', 'VBN')]),\n ImmutableTree('VP', [('point', 'VBP'), ('represents', 'VBZ')]),\n ImmutableTree('VP', [('shortchanged', 'VBN')]),\n ImmutableTree('VP', [('did', 'VBD')]),\n ImmutableTree('VP', [('established', 'VBN')]),\n ImmutableTree('VP', [('built', 'VBN')]),\n ImmutableTree('VP', [('simply', 'RB'), ('reflects', 'VBZ')]),\n ImmutableTree('VP', [('following', 'VBG')]),\n ImmutableTree('VP', [('are', 'VBP'), ('toiling', 'VBG')]),\n ImmutableTree('VP', [('have', 'VBP'), ('been', 'VBN'), ('trying', 'VBG')])]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).incorrect()[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.6%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "# c: Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of\n",
    "# this chapter.\n",
    "\n",
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt'[:100], chunk_types = ['VP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Exercise: 7-9\n",
    "# Sometimes a word is incorrectly tagged, e.g., the head noun in 12/CD or/CC so/RB cases/VBZ. Instead of requiring\n",
    "# manual correction of tagger output, good chunkers are able to work with the erroneous output of taggers. Look\n",
    "# for other examples of correctly chunked noun phrases with incorrect tags.\n",
    "\n",
    "misses = []\n",
    "\n",
    "for (i, sent) in enumerate(conll2000.chunked_sents('train.txt')):\n",
    "    for subtree in sent:\n",
    "        # only want subtrees, so use `try-except` to eliminate\n",
    "        # single nodes\n",
    "        try:\n",
    "            subtree.label()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if subtree.label() == 'NP':\n",
    "                # set flag only for NPs\n",
    "                flag = True\n",
    "                # exclude subtrees that have nouns, pronouns,\n",
    "                # numbers, relative pronouns, etc...\n",
    "                for leaf in subtree.leaves():\n",
    "                    if re.match(r'NN|PRP|CD|WP|EX|DT', leaf[1]):\n",
    "                        flag = False\n",
    "\n",
    "        # the flag will only still be True if the subtree is a NP\n",
    "        # and doesn't have a noun, pronoun, etc...\n",
    "        # if it does have a verb, though, we'll want to\n",
    "        # inspect it\n",
    "        if flag == True and re.match(r'VB.*', leaf[1]):\n",
    "            # print the id and the subtree\n",
    "            misses.append( (i, subtree) )\n",
    "            # reset flag\n",
    "            flag = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293, Tree('NP', [('estimates', 'VBZ')]))\n",
      "(324, Tree('NP', [(\"'s\", 'POS'), ('holding', 'VBG')]))\n",
      "(498, Tree('NP', [('operating', 'VBG'), ('results', 'VBZ')]))\n",
      "(524, Tree('NP', [(\"'s\", 'POS'), ('backing', 'VBG')]))\n",
      "(587, Tree('NP', [('offers', 'VBZ')]))\n",
      "(611, Tree('NP', [(\"'s\", 'VBZ')]))\n",
      "(725, Tree('NP', [(\"'s\", 'POS'), ('standing', 'VBG')]))\n",
      "(827, Tree('NP', [('around', 'IN'), ('$', '$'), ('5', 'VBG')]))\n",
      "(867, Tree('NP', [('trading', 'VBG')]))\n",
      "(876, Tree('NP', [('employees', 'VBZ')]))\n"
     ]
    }
   ],
   "source": [
    "for m in misses[:10]:\n",
    "    print(m)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP As/IN)\n",
      "  usual/JJ\n",
      "  ,/,\n",
      "  (NP estimates/VBZ)\n",
      "  (PP on/IN)\n",
      "  (NP the/DT fickle/JJ report/NN)\n",
      "  (VP are/VBP)\n",
      "  wide/JJ\n",
      "  ,/,\n",
      "  (VP running/VBG)\n",
      "  (PP from/IN)\n",
      "  (NP a/DT drop/NN)\n",
      "  (PP of/IN)\n",
      "  (NP 3.5/CD %/NN)\n",
      "  (PP to/TO)\n",
      "  (NP a/DT gain/NN)\n",
      "  (PP of/IN)\n",
      "  (NP 1.6/CD %/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt')[293])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Exercise: 7-10\n",
    "# The bigram chunker scores about 90% accuracy. Study its errors and try to work out why it doesn’t get 100%\n",
    "# accuracy. Experiment with trigram chunking. Are you able to improve the performance any more?\n",
    "\n",
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                       for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.3%%\n",
      "    Precision:     81.2%%\n",
      "    Recall:        86.2%%\n",
      "    F-Measure:     83.6%%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt')\n",
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[ImmutableTree('NP', [('Showtime', 'NNP')]),\n ImmutableTree('NP', [('beef', 'NN'), ('and', 'CC'), ('pork', 'NN')]),\n ImmutableTree('NP', [('him', 'PRP')]),\n ImmutableTree('NP', [('returns', 'NNS')]),\n ImmutableTree('PP', [('on', 'IN')]),\n ImmutableTree('NP', [('the', 'DT'), ('government', 'NN'), ('lawsuit', 'NN')]),\n ImmutableTree('VP', [('was', 'VBD'), ('dismissed', 'VBN')]),\n ImmutableTree('NP', [('Charges', 'NNS')]),\n ImmutableTree('PP', [('Of', 'IN')]),\n ImmutableTree('NP', [('no', 'DT'), ('plans', 'NNS')]),\n ImmutableTree('PP', [('with', 'IN')]),\n ImmutableTree('VP', [('told', 'VBD')]),\n ImmutableTree('VP', [('frequently', 'RB'), ('attempting', 'VBG'), ('to', 'TO'), ('enforce', 'VB')]),\n ImmutableTree('NP', [('the', 'DT'), ('company', 'NN')]),\n ImmutableTree('NP', [('Houston-based', 'NNP'), ('Maryland', 'NNP'), ('Club', 'NNP'), ('Foods', 'NNPS')]),\n ImmutableTree('VP', [('were', 'VBD')]),\n ImmutableTree('NP', [('Union', 'NNP'), ('Carbide', 'NNP')]),\n ImmutableTree('VP', [('accepting', 'VBG')]),\n ImmutableTree('PP', [('at', 'IN')]),\n ImmutableTree('PP', [('as', 'IN')])]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).missed()[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.evaluate(test_sents).incorrect()[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class TrigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                       for sent in train_sents]\n",
    "        self.tagger = nltk.TrigramTagger(train_data)\n",
    "\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     81.0%%\n",
      "    Recall:        84.4%%\n",
      "    F-Measure:     82.6%%\n"
     ]
    }
   ],
   "source": [
    "trigram_chunker = TrigramChunker(train_sents)\n",
    "print(trigram_chunker.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'word': 'Confidence', 'pos': 'NN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'in', 'pos': 'IN', 'trueiob': 'B-PP'}, 'B-PP'), ({'word': 'the', 'pos': 'DT', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'pound', 'pos': 'NN', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'is', 'pos': 'VBZ', 'trueiob': 'B-VP'}, 'B-VP'), ({'word': 'widely', 'pos': 'RB', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'expected', 'pos': 'VBN', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'to', 'pos': 'TO', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'take', 'pos': 'VB', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'another', 'pos': 'DT', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'sharp', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'dive', 'pos': 'NN', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'if', 'pos': 'IN', 'trueiob': 'O'}, 'O'), ({'word': 'trade', 'pos': 'NN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'figures', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'for', 'pos': 'IN', 'trueiob': 'B-PP'}, 'B-PP'), ({'word': 'September', 'pos': 'NNP', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': ',', 'pos': ',', 'trueiob': 'O'}, 'O'), ({'word': 'due', 'pos': 'JJ', 'trueiob': 'O'}, 'O'), ({'word': 'for', 'pos': 'IN', 'trueiob': 'B-PP'}, 'B-PP'), ({'word': 'release', 'pos': 'NN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'tomorrow', 'pos': 'NN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': ',', 'pos': ',', 'trueiob': 'O'}, 'O'), ({'word': 'fail', 'pos': 'VB', 'trueiob': 'B-VP'}, 'B-VP'), ({'word': 'to', 'pos': 'TO', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'show', 'pos': 'VB', 'trueiob': 'I-VP'}, 'I-VP'), ({'word': 'a', 'pos': 'DT', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'substantial', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'improvement', 'pos': 'NN', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'from', 'pos': 'IN', 'trueiob': 'B-PP'}, 'B-PP'), ({'word': 'July', 'pos': 'NNP', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'and', 'pos': 'CC', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'August', 'pos': 'NNP', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': \"'s\", 'pos': 'POS', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'near-record', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'deficits', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')] \n",
      "\n",
      "[({'word': 'Confidence', 'pos': 'NN', 'trueiob': 'B-NP'}, 'O'), ({'word': 'in', 'pos': 'IN', 'trueiob': 'B-PP'}, 'O'), ({'word': 'the', 'pos': 'DT', 'trueiob': 'B-NP'}, 'O'), ({'word': 'pound', 'pos': 'NN', 'trueiob': 'I-NP'}, 'O'), ({'word': 'is', 'pos': 'VBZ', 'trueiob': 'B-VP'}, 'O'), ({'word': 'widely', 'pos': 'RB', 'trueiob': 'I-VP'}, 'O'), ({'word': 'expected', 'pos': 'VBN', 'trueiob': 'I-VP'}, 'O'), ({'word': 'to', 'pos': 'TO', 'trueiob': 'I-VP'}, 'O'), ({'word': 'take', 'pos': 'VB', 'trueiob': 'I-VP'}, 'O'), ({'word': 'another', 'pos': 'DT', 'trueiob': 'B-NP'}, 'O'), ({'word': 'sharp', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'O'), ({'word': 'dive', 'pos': 'NN', 'trueiob': 'I-NP'}, 'O'), ({'word': 'if', 'pos': 'IN', 'trueiob': 'O'}, 'O'), ({'word': 'trade', 'pos': 'NN', 'trueiob': 'B-NP'}, 'O'), ({'word': 'figures', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'O'), ({'word': 'for', 'pos': 'IN', 'trueiob': 'B-PP'}, 'O'), ({'word': 'September', 'pos': 'NNP', 'trueiob': 'B-NP'}, 'O'), ({'word': ',', 'pos': ',', 'trueiob': 'O'}, 'O'), ({'word': 'due', 'pos': 'JJ', 'trueiob': 'O'}, 'O'), ({'word': 'for', 'pos': 'IN', 'trueiob': 'B-PP'}, 'O'), ({'word': 'release', 'pos': 'NN', 'trueiob': 'B-NP'}, 'O'), ({'word': 'tomorrow', 'pos': 'NN', 'trueiob': 'B-NP'}, 'O'), ({'word': ',', 'pos': ',', 'trueiob': 'O'}, 'O'), ({'word': 'fail', 'pos': 'VB', 'trueiob': 'B-VP'}, 'O'), ({'word': 'to', 'pos': 'TO', 'trueiob': 'I-VP'}, 'O'), ({'word': 'show', 'pos': 'VB', 'trueiob': 'I-VP'}, 'O'), ({'word': 'a', 'pos': 'DT', 'trueiob': 'B-NP'}, 'O'), ({'word': 'substantial', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'O'), ({'word': 'improvement', 'pos': 'NN', 'trueiob': 'I-NP'}, 'O'), ({'word': 'from', 'pos': 'IN', 'trueiob': 'B-PP'}, 'O'), ({'word': 'July', 'pos': 'NNP', 'trueiob': 'B-NP'}, 'O'), ({'word': 'and', 'pos': 'CC', 'trueiob': 'I-NP'}, 'O'), ({'word': 'August', 'pos': 'NNP', 'trueiob': 'I-NP'}, 'O'), ({'word': \"'s\", 'pos': 'POS', 'trueiob': 'B-NP'}, 'O'), ({'word': 'near-record', 'pos': 'JJ', 'trueiob': 'I-NP'}, 'O'), ({'word': 'deficits', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'O'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')] \n",
      "\n",
      "TBL train (fast) (seqs: 100; tokens: 2440; tpls: 50; min score: 2; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 15137 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      " 288 290   2  74  | O->I-NP if POS:NN@[0] & IOB:O@[0]\n",
      " 207 221  14  22  | O->B-NP if POS:IN@[-1] & IOB:O@[0]\n",
      " 169 218  49  10  | O->B-PP if POS:IN@[0] & IOB:O@[0]\n",
      " 145 146   1  53  | O->I-NP if POS:NNP@[0] & IOB:O@[0]\n",
      " 106 106   0  29  | O->I-NP if POS:NNS@[0] & IOB:O@[0]\n",
      " 101 104   3   7  | O->B-NP if POS:DT@[0] & IOB:O@[0]\n",
      "  75  75   0   2  | O->B-VP if POS:VBD@[0] & IOB:O@[0]\n",
      "  72  72   0  13  | O->I-VP if POS:VB@[0] & IOB:O@[0]\n",
      "  67  81  14  46  | O->I-NP if POS:NN@[1,2,3] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "  39  39   0   0  | O->B-VP if POS:VBP@[0] & IOB:O@[0]\n",
      "  39  39   0   0  | O->B-VP if POS:VBZ@[0] & IOB:O@[0]\n",
      "  34  34   0   0  | I-NP->B-NP if POS:IN@[-1] & IOB:I-NP@[0]\n",
      "  32  36   4   7  | O->I-NP if POS:CD@[0] & IOB:O@[0]\n",
      "  30  30   0   1  | O->B-NP if POS:PRP@[0] & IOB:O@[0]\n",
      "  29  30   1  13  | O->I-VP if POS:VBN@[0] & IOB:O@[0]\n",
      "  29  39  10  34  | O->B-VP if POS:VB@[1,2,3] & IOB:O@[0] & IOB:I-VP@[1]\n",
      "  22  23   1   0  | I-NP->B-NP if POS:VB@[-1] & IOB:I-NP@[0]\n",
      "  19  19   0  16  | O->I-NP if POS:JJ@[0] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "  20  22   2   0  | I-NP->B-NP if POS:VBD@[-1] & IOB:I-NP@[0]\n",
      "  18  18   0   5  | O->B-PP if POS:TO@[0] & IOB:O@[0]\n",
      "  14  14   0  18  | O->B-VP if POS:VBG@[0] & IOB:O@[0]\n",
      "  18  18   0   0  | B-VP->I-VP if POS:VB@[1,2,3] & IOB:B-VP@[-1] & IOB:B-VP@[0]\n",
      "  12  19   7   0  | I-NP->B-NP if POS:,@[-1] & IOB:I-NP@[0]\n",
      "   9  10   1   3  | B-NP->B-PP if POS:IN@[0] & IOB:B-NP@[0]\n",
      "   9   9   0   0  | I-NP->B-NP if POS:POS@[0] & IOB:I-NP@[0]\n",
      "   9  10   1   1  | B-VP->I-VP if POS:TO@[0] & IOB:I-VP@[-1] & IOB:B-VP@[0]\n",
      "   7   8   1   0  | I-NP->B-NP if POS:TO@[-1] & IOB:I-NP@[0]\n",
      "   7   8   1   0  | O->I-NP if POS:DT@[-1] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   7   8   1   0  | B-PP->O if POS:PRP@[1] & IOB:B-PP@[0]\n",
      "   7   7   0   0  | O->B-NP if POS:NNS@[1,2,3] & IOB:I-VP@[-1] & IOB:O@[0]\n",
      "   6   6   0   0  | B-VP->I-VP if POS:RB@[0] & IOB:B-VP@[0]\n",
      "   6   6   0   0  | O->B-VP if POS:MD@[0] & IOB:O@[0]\n",
      "   6   6   0   0  | I-NP->B-NP if POS:VBP@[1,2,3] & IOB:O@[-1] & IOB:I-NP@[0]\n",
      "   5   5   0   0  | B-VP->O if POS:CC@[0] & IOB:B-VP@[0]\n",
      "   6   6   0   1  | I-VP->B-VP if POS:NN@[-3,-2,-1] & IOB:O@[-1] & IOB:I-VP@[0]\n",
      "   5   5   0   0  | B-VP->I-VP if POS:VBG@[0] & IOB:B-VP@[-1] & IOB:B-VP@[0]\n",
      "   5   5   0   0  | I-VP->B-VP if POS:VBN@[0] & IOB:I-NP@[-1] & IOB:I-VP@[0]\n",
      "   5   9   4   1  | B-PP->O if POS:IN@[0] & IOB:B-PP@[0] & IOB:B-PP@[1]\n",
      "   5   5   0   1  | I-NP->B-NP if POS:VBZ@[-1] & IOB:I-NP@[0]\n",
      "   5   6   1   0  | I-NP->B-NP if POS:JJ@[0] & POS:NNP@[1] & IOB:I-NP@[0]\n",
      "   4   7   3   0  | I-NP->O if POS:,@[0] & IOB:I-NP@[0]\n",
      "   4   4   0   0  | O->B-NP if POS:EX@[0] & IOB:O@[0]\n",
      "   4   4   0   1  | O->B-NP if POS:POS@[0] & IOB:O@[0]\n",
      "   4   4   0   0  | O->B-NP if POS:WDT@[0] & IOB:O@[0]\n",
      "   4   5   1   1  | O->B-NP if POS:TO@[-1] & IOB:O@[0]\n",
      "   4   4   0   0  | O->I-NP if POS:RB@[-1] & IOB:B-NP@[-1] & IOB:O@[0]\n",
      "   4   4   0   1  | I-NP->B-NP if POS:NNS@[-3,-2,-1] & IOB:B-VP@[-1] &\n",
      "                  |   IOB:I-NP@[0]\n",
      "   4   5   1   0  | O->I-NP if POS:NNP@[-3,-2,-1] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   5   7   2   0  | I-NP->B-NP if POS:CC@[-1] & IOB:O@[-1] & IOB:I-NP@[0]\n",
      "   3   3   0   0  | B-NP->O if POS:,@[0] & IOB:B-NP@[0]\n",
      "   3   3   0   0  | B-VP->O if POS:,@[0] & IOB:B-VP@[0]\n",
      "   4   4   0   0  | I-VP->B-VP if POS:VB@[0] & IOB:O@[-1] & IOB:I-VP@[0]\n",
      "   3   3   0   0  | O->B-NP if POS:WP@[0] & IOB:O@[0]\n",
      "   3   3   0   0  | O->I-NP if POS:NNPS@[0] & IOB:O@[0]\n",
      "   3   3   0   0  | O->B-NP if POS:JJR@[0] & IOB:B-VP@[-1] & IOB:O@[0]\n",
      "   3   3   0   0  | B-PP->O if POS:IN@[0] & IOB:B-PP@[0] & IOB:O@[1]\n",
      "   3   3   0   1  | B-VP->I-VP if POS:RB@[-1] & IOB:B-VP@[-2] & IOB:O@[-1]\n",
      "   3   3   0   0  | O->I-VP if POS:VBG@[1] & IOB:O@[0] & IOB:I-VP@[1]\n",
      "   3   3   0   0  | I-NP->B-NP if POS:VBP@[-2] & POS:VBN@[-1] & IOB:I-NP@[0]\n",
      "   3   3   0   0  | I-NP->B-NP if POS:NNS@[1] & POS:VBG@[2] & IOB:I-NP@[0]\n",
      "   3   3   0   0  | I-NP->O if POS:(@[1,2,3] & IOB:I-NP@[0]\n",
      "   3   3   0   1  | B-VP->I-NP if POS:NNS@[1,2,3] & IOB:B-VP@[0] & IOB:I-NP@[1]\n",
      "   2   2   0   1  | B-NP->O if POS:JJS@[0] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-VP->O if POS:``@[0] & IOB:B-VP@[0]\n",
      "   2   2   0   0  | I-NP->B-VP if POS:VBP@[0] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | I-NP->B-VP if POS:VBZ@[0] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | O->B-NP if POS:PRP$@[0] & IOB:O@[0]\n",
      "   2   2   0   0  | I-NP->B-NP if POS:NN@[0] & IOB:B-VP@[-1] & IOB:I-NP@[0]\n",
      "   2   2   0   1  | I-VP->B-PP if POS:VBN@[0] & IOB:O@[-1] & IOB:I-VP@[0]\n",
      "   2   2   0   0  | I-VP->B-VP if POS:VB@[0] & IOB:O@[-1] & IOB:I-VP@[0]\n",
      "   2   2   0   0  | I-VP->B-VP if POS:VBN@[0] & IOB:B-NP@[-1] & IOB:I-VP@[0]\n",
      "   2   3   1   1  | B-NP->O if POS:DT@[0] & IOB:B-NP@[0] & IOB:O@[1]\n",
      "   3   3   0   0  | O->I-NP if POS:JJ@[-1] & POS:JJ@[1] & IOB:O@[0]\n",
      "   3   3   0   0  | O->I-NP if POS:IN@[-3,-2,-1] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   2   2   0   0  | O->B-NP if POS:RBR@[0] & IOB:O@[0] & IOB:B-NP@[1]\n",
      "   3   3   0   0  | B-NP->I-NP if POS:DT@[0] & IOB:B-NP@[-1] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-NP->B-VP if POS:VBG@[0] & IOB:I-NP@[-2] & IOB:B-PP@[-1]\n",
      "   2   3   1   0  | B-PP->O if POS:VBZ@[-1] & IOB:B-PP@[0]\n",
      "   2   3   1   0  | I-NP->B-NP if POS:``@[-1] & IOB:I-NP@[0]\n",
      "   2   2   0   1  | I-NP->B-NP if POS:VBG@[-1] & IOB:B-VP@[-1] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | I-VP->B-VP if POS:VBN@[-1] & IOB:B-VP@[-1] & IOB:I-VP@[0]\n",
      "   2   2   0   0  | B-PP->B-NP if POS:CC@[-1] & IOB:O@[-2] & IOB:O@[-1]\n",
      "   2   2   0   0  | B-PP->O if POS:JJS@[1] & IOB:B-PP@[0]\n",
      "   2   2   0   0  | O->B-PP if POS:IN@[1] & IOB:B-PP@[-2] & IOB:B-NP@[-1]\n",
      "   2   2   0   0  | I-NP->B-NP if POS:NN@[-1] & POS:NNP@[0] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | O->B-NP if POS:JJ@[0] & POS:DT@[1] & IOB:O@[0]\n",
      "   2   2   0   0  | B-NP->I-NP if POS:JJ@[0] & POS:NN@[1] & IOB:I-NP@[-1] &\n",
      "                  |   IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-PP->I-NP if POS:JJR@[-1] & POS:CD@[1] & IOB:B-PP@[0]\n",
      "   2   2   0   0  | B-NP->I-NP if POS:CD@[0] & IOB:I-NP@[-1] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-PP->O if POS:VBD@[-1] & POS:DT@[1] & IOB:I-NP@[-2] &\n",
      "                  |   IOB:B-VP@[-1]\n",
      "   2   2   0   0  | B-NP->O if POS:TO@[1] & POS:DT@[2] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | B-PP->O if POS:RB@[-1] & IOB:B-PP@[0] & IOB:O@[1]\n",
      "   2   2   0   0  | B-PP->O if POS:NNS@[1] & POS:MD@[2] & IOB:B-PP@[0]\n",
      "   2   2   0   0  | I-NP->B-NP if POS:NNP@[1] & POS:RB@[2] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | I-NP->O if POS:(@[-3,-2,-1] & IOB:I-NP@[0]\n",
      "   2   2   0   0  | B-NP->I-NP if POS:,@[-3,-2,-1] & IOB:O@[-2] & IOB:B-NP@[-1]\n",
      "   2   2   0   0  | B-PP->O if POS:VBZ@[1,2,3] & IOB:O@[-1] & IOB:B-PP@[0]\n",
      "   2   2   0   1  | O->B-NP if POS:CD@[1,2,3] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   2   2   0   0  | I-NP->B-NP if POS:IN@[1] & IOB:O@[-2] & IOB:I-NP@[-1]\n",
      "   2   2   0   0  | I-NP->O if POS:CC@[0] & IOB:I-NP@[0] & IOB:B-NP@[1]\n",
      "   2   2   0   0  | B-NP->I-NP if POS:CD@[1] & POS:CD@[2] & IOB:B-NP@[0]\n",
      "   2   2   0   0  | O->I-NP if POS:CC@[1,2,3] & IOB:O@[0] & IOB:I-NP@[1]\n",
      "   2   2   0   0  | B-PP->O if POS:DT@[1,2,3] & IOB:O@[-2] & IOB:B-NP@[-1]\n",
      "   2   2   0   0  | B-PP->O if POS:IN@[1,2,3] & IOB:I-VP@[-2] & IOB:I-VP@[-1]\n",
      "   2   2   0   0  | O->B-NP if POS:CD@[1,2,3] & IOB:I-NP@[-2] & IOB:B-VP@[-1]\n",
      "   2   4   2   0  | I-NP->B-NP if POS:DT@[1,2,3] & IOB:I-NP@[1] & IOB:B-VP@[2]\n",
      "   2   3   1   0  | I-NP->O if POS:NN@[1] & IOB:I-NP@[0] & IOB:B-NP@[1]\n",
      "[({'word': 'He', 'pos': 'PRP', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'talked', 'pos': 'VBD', 'trueiob': 'B-VP'}, 'B-VP'), ({'word': 'about', 'pos': 'IN', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': '20', 'pos': 'CD', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': 'minutes', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')]\n",
      "[({'word': 'He', 'pos': 'PRP', 'trueiob': 'B-NP'}, 'O'), ({'word': 'talked', 'pos': 'VBD', 'trueiob': 'B-VP'}, 'O'), ({'word': 'about', 'pos': 'IN', 'trueiob': 'B-NP'}, 'O'), ({'word': '20', 'pos': 'CD', 'trueiob': 'I-NP'}, 'O'), ({'word': 'minutes', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'O'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')]\n",
      "[({'word': 'He', 'pos': 'PRP', 'trueiob': 'B-NP'}, 'B-NP'), ({'word': 'talked', 'pos': 'VBD', 'trueiob': 'B-VP'}, 'B-VP'), ({'word': 'about', 'pos': 'IN', 'trueiob': 'B-NP'}, 'B-PP'), ({'word': '20', 'pos': 'CD', 'trueiob': 'I-NP'}, 'B-NP'), ({'word': 'minutes', 'pos': 'NNS', 'trueiob': 'I-NP'}, 'I-NP'), ({'word': '.', 'pos': '.', 'trueiob': 'O'}, 'O')]\n",
      "\n",
      "TEMPLATE STATISTICS (TRAIN)  27 templates, 107 rules)\n",
      "TRAIN (   2440 tokens) initial  2018 0.1730 final:   154 0.9369 \n",
      "#ID | Score (train) |  #Rules     | Template\n",
      "--------------------------------------------\n",
      "000 |  1230   0.660 |  32   0.299 | Template(POS([0]),IOB([0]))\n",
      "005 |   315   0.169 |  10   0.093 | Template(POS([-1]),IOB([0]))\n",
      "047 |   103   0.055 |   5   0.047 | Template(POS([1, 2, 3]),IOB([0]),IOB([1]))\n",
      "001 |    39   0.021 |  11   0.103 | Template(POS([0]),IOB([-1]),IOB([0]))\n",
      "046 |    33   0.018 |   4   0.037 | Template(POS([1, 2, 3]),IOB([-1]),IOB([0]))\n",
      "002 |    33   0.018 |   6   0.056 | Template(POS([0]),IOB([0]),IOB([1]))\n",
      "006 |    13   0.007 |   4   0.037 | Template(POS([-1]),IOB([-1]),IOB([0]))\n",
      "035 |    11   0.006 |   5   0.047 | Template(POS([1]),POS([2]),IOB([0]))\n",
      "041 |    10   0.005 |   2   0.019 | Template(POS([-3, -2, -1]),IOB([-1]),IOB([0]))\n",
      "010 |     9   0.005 |   2   0.019 | Template(POS([1]),IOB([0]))\n",
      "007 |     9   0.005 |   2   0.019 | Template(POS([-1]),IOB([0]),IOB([1]))\n",
      "042 |     7   0.004 |   2   0.019 | Template(POS([-3, -2, -1]),IOB([0]),IOB([1]))\n",
      "020 |     7   0.004 |   2   0.019 | Template(POS([0]),POS([1]),IOB([0]))\n",
      "048 |     6   0.003 |   3   0.028 | Template(POS([1, 2, 3]),IOB([-2]),IOB([-1]))\n",
      "025 |     5   0.003 |   2   0.019 | Template(POS([-1]),POS([1]),IOB([0]))\n",
      "012 |     5   0.003 |   2   0.019 | Template(POS([1]),IOB([0]),IOB([1]))\n",
      "008 |     5   0.003 |   2   0.019 | Template(POS([-1]),IOB([-2]),IOB([-1]))\n",
      "013 |     4   0.002 |   2   0.019 | Template(POS([1]),IOB([-2]),IOB([-1]))\n",
      "045 |     3   0.002 |   1   0.009 | Template(POS([1, 2, 3]),IOB([0]))\n",
      "030 |     3   0.002 |   1   0.009 | Template(POS([-2]),POS([-1]),IOB([0]))\n",
      "049 |     2   0.001 |   1   0.009 | Template(POS([1, 2, 3]),IOB([1]),IOB([2]))\n",
      "043 |     2   0.001 |   1   0.009 | Template(POS([-3, -2, -1]),IOB([-2]),IOB([-1]))\n",
      "040 |     2   0.001 |   1   0.009 | Template(POS([-3, -2, -1]),IOB([0]))\n",
      "028 |     2   0.001 |   1   0.009 | Template(POS([-1]),POS([1]),IOB([-2]),IOB([-1]))\n",
      "021 |     2   0.001 |   1   0.009 | Template(POS([0]),POS([1]),IOB([-1]),IOB([0]))\n",
      "015 |     2   0.001 |   1   0.009 | Template(POS([-1]),POS([0]),IOB([0]))\n",
      "003 |     2   0.001 |   1   0.009 | Template(POS([0]),IOB([-2]),IOB([-1]))\n",
      "\n",
      "UNUSED TEMPLATES (23)\n",
      "004 Template(POS([0]),IOB([1]),IOB([2]))\n",
      "009 Template(POS([-1]),IOB([1]),IOB([2]))\n",
      "011 Template(POS([1]),IOB([-1]),IOB([0]))\n",
      "014 Template(POS([1]),IOB([1]),IOB([2]))\n",
      "016 Template(POS([-1]),POS([0]),IOB([-1]),IOB([0]))\n",
      "017 Template(POS([-1]),POS([0]),IOB([0]),IOB([1]))\n",
      "018 Template(POS([-1]),POS([0]),IOB([-2]),IOB([-1]))\n",
      "019 Template(POS([-1]),POS([0]),IOB([1]),IOB([2]))\n",
      "022 Template(POS([0]),POS([1]),IOB([0]),IOB([1]))\n",
      "023 Template(POS([0]),POS([1]),IOB([-2]),IOB([-1]))\n",
      "024 Template(POS([0]),POS([1]),IOB([1]),IOB([2]))\n",
      "026 Template(POS([-1]),POS([1]),IOB([-1]),IOB([0]))\n",
      "027 Template(POS([-1]),POS([1]),IOB([0]),IOB([1]))\n",
      "029 Template(POS([-1]),POS([1]),IOB([1]),IOB([2]))\n",
      "031 Template(POS([-2]),POS([-1]),IOB([-1]),IOB([0]))\n",
      "032 Template(POS([-2]),POS([-1]),IOB([0]),IOB([1]))\n",
      "033 Template(POS([-2]),POS([-1]),IOB([-2]),IOB([-1]))\n",
      "034 Template(POS([-2]),POS([-1]),IOB([1]),IOB([2]))\n",
      "036 Template(POS([1]),POS([2]),IOB([-1]),IOB([0]))\n",
      "037 Template(POS([1]),POS([2]),IOB([0]),IOB([1]))\n",
      "038 Template(POS([1]),POS([2]),IOB([-2]),IOB([-1]))\n",
      "039 Template(POS([1]),POS([2]),IOB([1]),IOB([2]))\n",
      "044 Template(POS([-3, -2, -1]),IOB([1]),IOB([2]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 7-11\n",
    "# Apply the n-gram and Brill tagging methods to IOB chunk tagging. Instead of assigning POS tags to words,\n",
    "# here we will assign IOB tags to the POS tags. E.g., if the tag DT (determiner) often occurs at the start of a\n",
    "# chunk, it will be tagged B (begin). Evaluate the performance of these chunking methods relative to the regular\n",
    "# expression chunking methods covered in this chapter.\n",
    "\n",
    "import sys\n",
    "\n",
    "from nltk import tbl, untag\n",
    "from nltk.tag.brill_trainer import BrillTaggerTrainer\n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.chunk.util import tree2conlltags\n",
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "def get_templates():\n",
    "\n",
    "    pos10 = [[POS([0])],\n",
    "             [POS([-1])],\n",
    "             [POS([1])],\n",
    "             [POS([-1]), POS([0])],\n",
    "             [POS([0]), POS([1])],\n",
    "             [POS([-1]), POS([1])],\n",
    "             [POS([-2]), POS([-1])],\n",
    "             [POS([1]), POS([2])],\n",
    "             [POS([-1, -2, -3])],\n",
    "             [POS([1, 2, 3])]]\n",
    "\n",
    "    iobs5 = [[IOB([0])],\n",
    "             [IOB([-1]), IOB([0])],\n",
    "             [IOB([0]), IOB([1])],\n",
    "             [IOB([-2]), IOB([-1])],\n",
    "             [IOB([1]), IOB([2])]]\n",
    "\n",
    "    templates = [tbl.Template(*pos + iob) for pos in pos10 for iob in iobs5]\n",
    "\n",
    "    return templates\n",
    "\n",
    "def build_multifeature_corpus():\n",
    "\n",
    "    def tuple2dict_featureset(sent, tagnames = (\"word\", \"pos\", \"trueiob\")):\n",
    "        return (dict(zip(tagnames, t)) for t in sent)\n",
    "\n",
    "    def tag_tokens(tokens):\n",
    "        return [(t, t[\"trueiob\"]) for t in tokens]\n",
    "\n",
    "    train_sents = conll2000.chunked_sents('train.txt')\n",
    "    conlltagged_sents = (tree2conlltags(sent)\n",
    "                        for sent in train_sents)\n",
    "    conlltagged_tokens = (tuple2dict_featureset(sent)\n",
    "                        for sent in conlltagged_sents)\n",
    "    conlltagged_sequences = (tag_tokens(sent)\n",
    "                            for sent in conlltagged_tokens)\n",
    "\n",
    "    return conlltagged_sequences\n",
    "\n",
    "class POS(tbl.Feature):\n",
    "    @staticmethod\n",
    "    def extract_property(tokens, index):\n",
    "        return tokens[index][0][\"pos\"]\n",
    "\n",
    "class IOB(tbl.Feature):\n",
    "    @staticmethod\n",
    "    def extract_property(tokens, index):\n",
    "        return tokens[index][1]\n",
    "\n",
    "\n",
    "\n",
    "class MyInitialTagger(DefaultTagger):\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        tokens_ = [t[\"word\"] for t in tokens]\n",
    "        return super().choose_tag(tokens_, index, history)\n",
    "\n",
    "\n",
    "templates = get_templates()\n",
    "trainon = 100\n",
    "\n",
    "corpus = list(build_multifeature_corpus())\n",
    "train, test = corpus[:trainon], corpus[trainon:]\n",
    "\n",
    "print(train[0], \"\\n\")\n",
    "\n",
    "initial_tagger = MyInitialTagger('O')\n",
    "print(initial_tagger.tag(untag(train[0])), \"\\n\")\n",
    "\n",
    "trainer = BrillTaggerTrainer(initial_tagger, templates, trace = 3)\n",
    "tagger = trainer.train(train)\n",
    "\n",
    "taggedtest = tagger.tag_sents([untag(t) for t in test])\n",
    "print(test[0])\n",
    "print(initial_tagger.tag(untag(test[0])))\n",
    "print(taggedtest[0])\n",
    "print()\n",
    "\n",
    "tagger.print_template_statistics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "0.35955948727207077"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 7-12\n",
    "# We saw in Chapter 5 that it is possible to establish an upper limit to tagging performance by looking for\n",
    "# ambiguous n-grams, which are n-grams that are tagged in more than one possible way in the training data. Apply the\n",
    "# same method to determine an upper bound on the performance of an n-gram chunker.\n",
    "\n",
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[2], y[2], z[0]), z[2])\n",
    "           for sent in train_sents\n",
    "           for x, y, z in nltk.trigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "0.22214427565573983"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[1], y[1], z[0]), z[2])\n",
    "           for sent in train_sents\n",
    "           for x, y, z in nltk.trigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "0.15939442395481393"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[1], x[2], y[1], y[2], z[0]), z[2])\n",
    "           for sent in train_sents\n",
    "           for x, y, z in nltk.trigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0.43632113851206417"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[2], y[0]), y[2])\n",
    "           for sent in train_sents\n",
    "           for x, y in nltk.bigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3850516048542588"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[1], y[0]), y[2])\n",
    "           for sent in train_sents\n",
    "           for x, y in nltk.bigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "0.27115601777199183"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[1], x[2], y[0]), y[2])\n",
    "           for sent in train_sents\n",
    "           for x, y in nltk.bigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "0.005591519872075929"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt')\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           ((x[0], x[1], x[2], y[0], y[1], y[2], z[0]), z[2])\n",
    "           for sent in train_sents\n",
    "           for x, y, z in nltk.trigrams(tree2conlltags(sent)))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "2283"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 7-13\n",
    "# Pick one of the three chunk types in the CoNLL Chunking Corpus. Write functions to do the following tasks for\n",
    "# your chosen type:\n",
    "# a: List all the tag sequences that occur with each instance of this chunk type.\n",
    "\n",
    "np_tags = []\n",
    "\n",
    "for (i, sent) in enumerate(conll2000.chunked_sents('train.txt')):\n",
    "    for subtree in sent:\n",
    "        # only want subtrees, so use `try-except` to eliminate\n",
    "        # single nodes\n",
    "        try:\n",
    "            subtree.label()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            if subtree.label() == 'NP':\n",
    "                # concatenating a string with all the POS tags\n",
    "                subtree_tag = \"\"\n",
    "                for t in tree2conlltags(subtree):\n",
    "                    if subtree_tag == \"\":\n",
    "                        subtree_tag += t[1]\n",
    "                    else:\n",
    "                        subtree_tag += \"/\" + t[1]\n",
    "                np_tags.append(subtree_tag)\n",
    "\n",
    "len(set(np_tags))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7223 DT/NN\n",
      "3802 PRP\n",
      "3282 NNS\n",
      "3249 NNP\n",
      "3245 NN\n",
      "2642 NNP/NNP\n",
      "2119 DT/JJ/NN\n",
      "1722 JJ/NNS\n",
      "1173 DT/NNS\n",
      "1143 JJ/NN\n",
      "1012 NN/NNS\n",
      " 930 WDT\n",
      " 921 DT/NN/NN\n",
      " 866 CD\n",
      " 830 CD/NN\n",
      " 824 $/CD/CD\n",
      " 690 CD/NNS\n",
      " 677 NNP/NNP/NNP\n",
      " 624 PRP$/NN\n",
      " 552 POS/NN\n",
      " 540 DT\n",
      " 509 WP\n",
      " 463 DT/NNP\n",
      " 454 NN/NN\n",
      " 446 $/CD\n",
      " 399 DT/NNP/NN\n",
      " 355 PRP$/NNS\n",
      " 313 JJ/NN/NNS\n",
      " 311 DT/NNP/NNP\n",
      " 277 DT/JJ/NN/NN\n",
      " 276 DT/JJ/NNS\n",
      " 220 NNP/NNP/NNP/NNP\n",
      " 204 POS/NNS\n",
      " 200 CD/CD\n",
      " 195 PRP$/JJ/NN\n",
      " 189 EX\n",
      " 183 NNP/CD\n",
      " 183 NNP/NNS\n",
      " 182 DT/JJ/JJ/NN\n",
      " 171 POS/JJ/NN\n",
      " 161 DT/NN/NNS\n",
      " 158 DT/NNP/NNP/NNP\n",
      " 149 JJ/JJ/NN\n",
      " 146 JJ/JJ/NNS\n",
      " 141 DT/VBN/NN\n",
      " 124 DT/CD/NNS\n",
      " 124 IN\n",
      " 115 NNP/NN\n",
      " 113 NNP/CC/NNP\n",
      " 113 POS/NN/NN\n"
     ]
    }
   ],
   "source": [
    "# b: Count the frequency of each tag sequence, and produce a ranked list in order of decreasing frequency;\n",
    "# each line should consist of an integer (the frequency) and the tag sequence.\n",
    "\n",
    "fd = nltk.FreqDist(np_tags)\n",
    "for tag, value in fd.most_common(50):\n",
    "    print(\"{:>4} {}\".format(value, tag))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.6%%\n",
      "    Precision:     71.9%%\n",
      "    Recall:        74.1%%\n",
      "    F-Measure:     73.0%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PRP$|POS|$|WP|EX>?<JJ.*>*<CD>*<NN.*|WDT>*<CD>*}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt'[:100], chunk_types = ['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 7-16\n",
    "# The Penn Treebank Corpus sample contains a section of tagged Wall Street Journal text that has been\n",
    "# chunked into noun phrases. The format uses square brackets, and we have encountered it several times in this\n",
    "# chapter. The corpus can be accessed using: for sent in nltk.corpus.treebank_chunk.chunked_sents(fileid).\n",
    "# These are flat trees, just as we got using nltk.corpus.conll2000.chunked_sents().\n",
    "# a: The functions nltk.tree.pprint() and nltk.chunk.tree2conllstr() can be used to create Treebank and IOB\n",
    "# strings from a tree. Write functions chunk2brackets() and chunk2iob() that take a single chunk tree as their\n",
    "# sole argument, and return the required multiline string representation.\n",
    "\n",
    "nltk.tree.Tree.pprint(nltk.corpus.treebank_chunk.chunked_sents()[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.treebank_chunk.chunked_sents()[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Mr./NNP Vinken/NNP)\n",
      "  is/VBZ\n",
      "  (NP chairman/NN)\n",
      "  of/IN\n",
      "  (NP Elsevier/NNP N.V./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT Dutch/NNP publishing/VBG group/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Mr./NNP Vinken/NNP)\n",
      "  is/VBZ\n",
      "  (NP chairman/NN)\n",
      "  of/IN\n",
      "  (NP Elsevier/NNP N.V./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT Dutch/NNP publishing/VBG group/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Rudolph/NNP Agnew/NNP)\n",
      "  ,/,\n",
      "  (NP 55/CD years/NNS)\n",
      "  old/JJ\n",
      "  and/CC\n",
      "  (NP former/JJ chairman/NN)\n",
      "  of/IN\n",
      "  (NP Consolidated/NNP Gold/NNP Fields/NNP PLC/NNP)\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  named/VBN\n",
      "  (NP a/DT nonexecutive/JJ director/NN)\n",
      "  of/IN\n",
      "  (NP this/DT British/JJ industrial/JJ conglomerate/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Rudolph/NNP Agnew/NNP)\n",
      "  ,/,\n",
      "  (NP 55/CD years/NNS)\n",
      "  old/JJ\n",
      "  and/CC\n",
      "  (NP former/JJ chairman/NN)\n",
      "  of/IN\n",
      "  (NP Consolidated/NNP Gold/NNP Fields/NNP PLC/NNP)\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  named/VBN\n",
      "  (NP a/DT nonexecutive/JJ director/NN)\n",
      "  of/IN\n",
      "  (NP this/DT British/JJ industrial/JJ conglomerate/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT form/NN)\n",
      "  of/IN\n",
      "  (NP asbestos/NN)\n",
      "  once/RB\n",
      "  used/VBN\n",
      "  to/TO\n",
      "  make/VB\n",
      "  Kent/NNP\n",
      "  (NP cigarette/NN filters/NNS)\n",
      "  has/VBZ\n",
      "  caused/VBN\n",
      "  (NP a/DT high/JJ percentage/NN)\n",
      "  of/IN\n",
      "  (NP cancer/NN deaths/NNS)\n",
      "  among/IN\n",
      "  (NP a/DT group/NN)\n",
      "  of/IN\n",
      "  (NP workers/NNS)\n",
      "  exposed/VBN\n",
      "  to/TO\n",
      "  (NP it/PRP)\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  (NP 30/CD years/NNS)\n",
      "  ago/IN\n",
      "  ,/,\n",
      "  (NP researchers/NNS)\n",
      "  reported/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT form/NN)\n",
      "  of/IN\n",
      "  (NP asbestos/NN)\n",
      "  once/RB\n",
      "  used/VBN\n",
      "  to/TO\n",
      "  make/VB\n",
      "  Kent/NNP\n",
      "  (NP cigarette/NN filters/NNS)\n",
      "  has/VBZ\n",
      "  caused/VBN\n",
      "  (NP a/DT high/JJ percentage/NN)\n",
      "  of/IN\n",
      "  (NP cancer/NN deaths/NNS)\n",
      "  among/IN\n",
      "  (NP a/DT group/NN)\n",
      "  of/IN\n",
      "  (NP workers/NNS)\n",
      "  exposed/VBN\n",
      "  to/TO\n",
      "  (NP it/PRP)\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  (NP 30/CD years/NNS)\n",
      "  ago/IN\n",
      "  ,/,\n",
      "  (NP researchers/NNS)\n",
      "  reported/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT asbestos/NN fiber/NN)\n",
      "  ,/,\n",
      "  (NP crocidolite/NN)\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  unusually/RB\n",
      "  resilient/JJ\n",
      "  once/IN\n",
      "  (NP it/PRP)\n",
      "  enters/VBZ\n",
      "  (NP the/DT lungs/NNS)\n",
      "  ,/,\n",
      "  with/IN\n",
      "  (NP even/RB brief/JJ exposures/NNS)\n",
      "  to/TO\n",
      "  (NP it/PRP)\n",
      "  causing/VBG\n",
      "  (NP symptoms/NNS)\n",
      "  (NP that/WDT)\n",
      "  show/VBP\n",
      "  up/IN\n",
      "  (NP decades/NNS)\n",
      "  later/JJ\n",
      "  ,/,\n",
      "  (NP researchers/NNS)\n",
      "  said/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT asbestos/NN fiber/NN)\n",
      "  ,/,\n",
      "  (NP crocidolite/NN)\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  unusually/RB\n",
      "  resilient/JJ\n",
      "  once/IN\n",
      "  (NP it/PRP)\n",
      "  enters/VBZ\n",
      "  (NP the/DT lungs/NNS)\n",
      "  ,/,\n",
      "  with/IN\n",
      "  (NP even/RB brief/JJ exposures/NNS)\n",
      "  to/TO\n",
      "  (NP it/PRP)\n",
      "  causing/VBG\n",
      "  (NP symptoms/NNS)\n",
      "  (NP that/WDT)\n",
      "  show/VBP\n",
      "  up/IN\n",
      "  (NP decades/NNS)\n",
      "  later/JJ\n",
      "  ,/,\n",
      "  (NP researchers/NNS)\n",
      "  said/VBD\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Lorillard/NNP Inc./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT unit/NN)\n",
      "  of/IN\n",
      "  (NP New/JJ York-based/JJ Loews/NNP Corp./NNP)\n",
      "  (NP that/WDT)\n",
      "  makes/VBZ\n",
      "  Kent/NNP\n",
      "  (NP cigarettes/NNS)\n",
      "  ,/,\n",
      "  stopped/VBD\n",
      "  using/VBG\n",
      "  (NP crocidolite/NN)\n",
      "  in/IN\n",
      "  (NP its/PRP$ Micronite/NN cigarette/NN filters/NNS)\n",
      "  in/IN\n",
      "  (NP 1956/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP Lorillard/NNP Inc./NNP)\n",
      "  ,/,\n",
      "  (NP the/DT unit/NN)\n",
      "  of/IN\n",
      "  (NP New/JJ York-based/JJ Loews/NNP Corp./NNP)\n",
      "  (NP that/WDT)\n",
      "  makes/VBZ\n",
      "  Kent/NNP\n",
      "  (NP cigarettes/NNS)\n",
      "  ,/,\n",
      "  stopped/VBD\n",
      "  using/VBG\n",
      "  (NP crocidolite/NN)\n",
      "  in/IN\n",
      "  (NP its/PRP$ Micronite/NN cigarette/NN filters/NNS)\n",
      "  in/IN\n",
      "  (NP 1956/CD)\n",
      "  ./.)\n",
      "(S\n",
      "  Although/IN\n",
      "  (NP preliminary/JJ findings/NNS)\n",
      "  were/VBD\n",
      "  reported/VBN\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  (NP a/DT year/NN)\n",
      "  ago/IN\n",
      "  ,/,\n",
      "  (NP the/DT latest/JJS results/NNS)\n",
      "  appear/VBP\n",
      "  in/IN\n",
      "  (NP today/NN 's/POS New/NNP England/NNP Journal/NNP)\n",
      "  of/IN\n",
      "  (NP Medicine/NNP)\n",
      "  ,/,\n",
      "  (NP a/DT forum/NN)\n",
      "  likely/JJ\n",
      "  to/TO\n",
      "  bring/VB\n",
      "  (NP new/JJ attention/NN)\n",
      "  to/TO\n",
      "  (NP the/DT problem/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  Although/IN\n",
      "  (NP preliminary/JJ findings/NNS)\n",
      "  were/VBD\n",
      "  reported/VBN\n",
      "  more/RBR\n",
      "  than/IN\n",
      "  (NP a/DT year/NN)\n",
      "  ago/IN\n",
      "  ,/,\n",
      "  (NP the/DT latest/JJS results/NNS)\n",
      "  appear/VBP\n",
      "  in/IN\n",
      "  (NP today/NN 's/POS New/NNP England/NNP Journal/NNP)\n",
      "  of/IN\n",
      "  (NP Medicine/NNP)\n",
      "  ,/,\n",
      "  (NP a/DT forum/NN)\n",
      "  likely/JJ\n",
      "  to/TO\n",
      "  bring/VB\n",
      "  (NP new/JJ attention/NN)\n",
      "  to/TO\n",
      "  (NP the/DT problem/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP We/PRP)\n",
      "  're/VBP\n",
      "  talking/VBG\n",
      "  about/IN\n",
      "  (NP years/NNS)\n",
      "  ago/IN\n",
      "  before/IN\n",
      "  (NP anyone/NN)\n",
      "  heard/VBD\n",
      "  of/IN\n",
      "  (NP asbestos/NN)\n",
      "  having/VBG\n",
      "  (NP any/DT questionable/JJ properties/NNS)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP We/PRP)\n",
      "  're/VBP\n",
      "  talking/VBG\n",
      "  about/IN\n",
      "  (NP years/NNS)\n",
      "  ago/IN\n",
      "  before/IN\n",
      "  (NP anyone/NN)\n",
      "  heard/VBD\n",
      "  of/IN\n",
      "  (NP asbestos/NN)\n",
      "  having/VBG\n",
      "  (NP any/DT questionable/JJ properties/NNS)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP There/EX)\n",
      "  is/VBZ\n",
      "  (NP no/DT asbestos/NN)\n",
      "  in/IN\n",
      "  (NP our/PRP$ products/NNS)\n",
      "  now/RB\n",
      "  ./.)\n",
      "(S\n",
      "  (NP There/EX)\n",
      "  is/VBZ\n",
      "  (NP no/DT asbestos/NN)\n",
      "  in/IN\n",
      "  (NP our/PRP$ products/NNS)\n",
      "  now/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for sent in nltk.corpus.treebank_chunk.chunked_sents()[:10]:\n",
    "    nltk.tree.Tree.pprint(sent)\n",
    "    print(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre NNP B-NP\n",
      "Vinken NNP I-NP\n",
      ", , O\n",
      "61 CD B-NP\n",
      "years NNS I-NP\n",
      "old JJ O\n",
      ", , O\n",
      "will MD O\n",
      "join VB O\n",
      "the DT B-NP\n",
      "board NN I-NP\n",
      "as IN O\n",
      "a DT B-NP\n",
      "nonexecutive JJ I-NP\n",
      "director NN I-NP\n",
      "Nov. NNP I-NP\n",
      "29 CD I-NP\n",
      ". . O\n"
     ]
    }
   ],
   "source": [
    "print(nltk.chunk.tree2conllstr(nltk.corpus.treebank_chunk.chunked_sents()[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  (PP for/IN)\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt')[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# b: Write command-line conversion utilities bracket2iob.py and iob2bracket.py that take a file in Treebank or\n",
    "# CoNLL format (respectively) and convert it to the other format. (Obtain some raw Treebank or CoNLL data from the\n",
    "# NLTK Corpora, save it to a file, and then use for line in open(filename) to access it from Python.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}