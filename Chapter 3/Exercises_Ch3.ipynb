{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'colourless'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-1\n",
    "# Define a string s = 'colorless'. Write a Python statement that changes this to “colourless” using only the slice\n",
    "# and concatenation operations.\n",
    "\n",
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish', 'run', 'nation', 'un', 'pre']\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-2\n",
    "# We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last\n",
    "# character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we’ve inserted a\n",
    "# hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality,\n",
    "# un-do, pre-heat.\n",
    "\n",
    "affixed = [('dishes', 2),\n",
    "           ('running', 4),\n",
    "           ('nationality', 5),\n",
    "           ('undo', 2),\n",
    "           ('preheat', 4)]\n",
    "\n",
    "print([s[:-a] for s, a in affixed])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l\n",
      "a\n",
      "i\n",
      "r\n",
      "t\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-366ca736af91>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mtrial\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"trial\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-3\n",
    "# We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an\n",
    "# index that goes too far to the left, before the start of the string?\n",
    "\n",
    "trial = \"trial\"\n",
    "for i in range(1, len(trial) + 2):\n",
    "    print(trial[-i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\"MnyPto' ligCru\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-4\n",
    "# We can specify a “step” size for the slice. The following returns every second character within the slice:\n",
    "# monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2]. Try these for yourself, and then\n",
    "# experiment with different step values.\n",
    "\n",
    "tt = \"Monty Python's Flying Circus\"\n",
    "\n",
    "# Every other letter\n",
    "tt[::2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'sci nyFsnhy to'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[::-2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "\"MtPh'Fi rs\""
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[::3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'ytnom'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-5\n",
    "# What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result.\n",
    "\n",
    "\"monty\"[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{cAMELCASE} 6186258313 {hybr}1{d}\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-6\n",
    "# Describe the class of strings matched by the following regular expressions:\n",
    "# a: [a-zA-Z]+\n",
    "# b: [A-Z][a-z]*\n",
    "# c: p[aeiou]{,2}t\n",
    "# d: \\d+(\\.\\d+)?\n",
    "# e: ([^aeiou][aeiou][^aeiou])*\n",
    "# f: \\w+|[^\\w\\s]+\n",
    "# Test your answers using nltk.re_show().\n",
    "\n",
    "import nltk, re\n",
    "nltk.re_show(r'[a-zA-Z]+', \"cAMELCASE 6186258313 hybr1d\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c{A}{M}{E}{L}{C}{A}{S}{E} 6186258313 hybr1d\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'[A-Z][a-z]*', \"cAMELCASE 6186258313 hybr1d\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cAMELCASE 6186258313 hybr1d\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'p[aeiou]{,2}t', \"cAMELCASE 6186258313 hybr1d\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cAMELCASE {6186258313} hybr{1}d\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(\\.\\d+)?', \"cAMELCASE 6186258313 hybr1d\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}c{}A{}M{}E{}L{}C{}A{}S{}E{} {}6{}1{}8{}6{}2{}5{}8{}3{}1{}3{} {}h{}y{}b{}r{}1{}d{}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', \"cAMELCASE 6186258313 hybr1d\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{cAMELCASE} {6186258313} {hybr1d}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\w+|[^\\w\\s]+', \"cAMELCASE 6186258313 hybr1d\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think {a} relevant string like {the} one here is {an} example of what we need.\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-7\n",
    "# Write regular expressions to match the following classes of strings:\n",
    "# a: A single determiner (assume that a, an, and the are the only determiners)\n",
    "# b: An arithmetic expression using integers, addition, and multiplication, such as 2*3+8\n",
    "\n",
    "string = \"I think a relevant string like the one here is an example of what we need.\"\n",
    "nltk.re_show(r'\\b[Aa]n?\\b|\\b[Tt]he\\b', string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2 * 3 + 8}\n"
     ]
    }
   ],
   "source": [
    "string = \"2 * 3 + 8\"\n",
    "nltk.re_show(r'(\\d|[+*= ])+', string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Exercise: 3-8\n",
    "# Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML\n",
    "# markup removed. Use urllib.urlopen to access the contents of the URL, e.g.:\n",
    "# raw_contents = urllib.urlopen('http://www.nltk.org/').read()\n",
    "\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize\n",
    "\n",
    "def return_URL_contents(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html, 'html.parser')\n",
    "    for r in raw(['script', 'style']):\n",
    "        r.extract() # remove tags\n",
    "\n",
    "    text = ' '.join(raw.stripped_strings) # retrieve tag content\n",
    "\n",
    "    return normalize('NFKD', text) # normalize escape sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'What Virtual Reality Can Teach a Driverless Car - The New York Times Sections SEARCH Skip to content Skip to site index Business Log in Today’s Paper Business | What Virtual Reality Can Teach a Driverless Car https://www.nytimes.com/2017/10/29/business/virtual-reality-driverless-cars.html Today in Business live Latest Updates What Apple Is Thinking Airlines See Fast Recovery Welcome to the YOLO Economy Advertisement Continue reading the main story Supported by Continue reading the main story What Virtual Reality Can Teach a Driverless Car By Cade Metz Oct. 29, 2017 SAN FRANCISCO — As the computers that operate driverless cars digest the rules of the road, some engineers think it might be nice if they can learn from mistakes made in virtual reality rather than on real streets. Companies like Toyota, Uber and Waymo have discussed at length how they are testing autonomous vehicles on the streets of Mountain View, Calif., Phoenix and other cities. What is not as well known is that they are also testing vehicles inside computer simulations of these same cities. Virtual cars, equipped with the same software as the real thing, spend thousands of hours driving their digital worlds. Think of it as a way of identifying flaws in the way the cars operate without endangering real people. If a car makes a mistake on a simulated drive, engineers can tweak its software accordingly, laying down new rules of behavior. On Monday, Waymo, the autonomous car company that spun out of Google, is expected to show off its simulator tests when it takes a group of reporters to its secretive testing center in California’s Central Valley. Researchers are also developing methods that would allow cars to actually learn new behavior from these simulations, gathering skills more quickly than human engineers could ever lay them down with explicit software code. “Simulation is a tremendous thing,” said Gill Pratt, chief executive of the Toyota Research Institute, one of the artificial intelligence lab'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.nytimes.com/2017/10/29/business/virtual-reality-driverless-cars.html?module=inline\"\n",
    "return_URL_contents(url)[:2000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Exercise: 3-9\n",
    "# Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole\n",
    "# argument, and returns a string containing the text of the file.\n",
    "# a: Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text.\n",
    "# Use one multiline regular expression inline comments, using the verbose flag (?x).\n",
    "\n",
    "def load(f):\n",
    "    text = open(f, encoding = \"utf-8\")\n",
    "    raw = text.read()\n",
    "\n",
    "    return raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '(', ')', ',', '.', ':', '.', ',', '.', ',', '.', '.', ',', '.', ',', ',', ';', '.', ',', '.', ',', ',', '.', '(', ')', ',', ',', ',', '.', ',', '.', ',', ',', ':', '.', ',', ',', ',', '.', ',', ',', '.', '.', ':', '(', ')', ',', '(', ')', ',', '.', \"'\", \"'\", ',', '.', \"'\", \"'\", ',', ',', '.', ',', '.', \"'\", '.', ',', '.', ',', '.', ',', ',', '.', ',', '.', ',', ',', '.', ',', '.', ',', '.', '.', '.', '\"', '\"', '.', '\"', '\"', ',', '.', '\"', '\"', '.', '\"', '\"', '.', '\"', '\"', '\"', '\"', '.', '\"', '\"', ',', '.', '\"', '\"', '\"', '\"', '.', '\"', '\"', '.', '\"', '\"', ',', ',', ',', '.', ',', '(', ')', ',', ',', '.', '\"', '\"', '.', ',', ',', '.', '\"', '\"', '(', ')', ',', '(', ')', '(', ')', ',', ',', '.', ',', ',', '.', '.', '.', '\"', '\"', '.', '\"', '\"', '.', '\"', '\"', ',', ',', ',', '.', '\"', '\"', ',', ',', '(', ')', ',', ',', '(', ')', ',', '.', '\"', '\"', ',', ',', '(', ',', ',', ')', '(', ')', ',', ',', '.', '\"', '\"', ',', ',', '(', ')', ',', '.', ',', \"'\", ',', '.', ',', ',', ',', '.', '.', '.', '.', '.', ',', '.', '.', ',', ',', '.', ',', '.', ',', ',', '.', ',', ',', '.', ',', ',', '.', '.', ';', '.', '.', \"'\", '.', ',', '.', ',', ',', ',', \"'\", ',', \"'\", '.', '.', '.', \"'\", ',', ',', ';', ';', ';', '.', ',', '.', '.', '.', ',', ',', ',', ':', ')', ',', '.', ')', '.', '\"', '\"', '.', ')', ',', ',', '.', ',', ',', ',', ',', '.', ',', '.', ')', ',', ';', ',', ',', '.', ',', ',', ',', ',', '\"', '\"', \"'\", '.', '.', '.', '.', ',', ',', ':', ')', ',', ',', '(', ')', ',', '.', ')', ',', ',', '(', ')', ',', ',', ',', '(', ')', ',', ',', ',', '(', ')', '.', ')', '.', ',', ',', '.', ')', '(', ')', ',', '.', '.', ',', '(', ')', ',', '.', ',', '.', ')', ',', '.', ',', ',', '.', '\"', '\"', '(', ')', '\"', '\"', ',', ',', ',', ',', '(', ')', '.', ',', '.', ',', '\"', '\"', ',', ',', ',', '.', ',', ',', '.', '\"', '\"', ',', ',', ',', '.', '.', ',', ',', ',', ',', '(', ')', ',', '.', '(', ',', ')', '.', ',', ',', ',', '.', '.', ',', ',', '(', ')', ',', ',', '.', '.', '.', '\"', '\"', '.', ',', '.', ',', ',', '.', ',', ',', '.', '(', '.', ')', ',', ',', '.', ',', ',', '(', ')', ':', ')', ';', ')', ';', ')', ',', ';', ')', ';', ')', ',', ',', ';', ')', '(', ')', ',', '.', '\"', '\"', '.', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', ',', '.', ',', ',', ',', ';', '.', '.', '.', '.', ',', '(', ')', '.', ',', ',', '(', ')', ',', ',', '(', ')', ',', '.', ',', ',', '(', ')', ',', '.', '.', ',', '.', '.', '.', '.', '.', ',', '.', '.', ',', ',', '.', '.', '.', ',', ',', ',', ',', '.', '.', '\"', '\"', ',', ',', ',', '.', ',', \"'\", ',', ',', '.', '.', ',', ',', ',', ',', '(', ')', ',', ',', ',', ',', '.', '.', '.', '\"', '\"', '.', \"'\", '\"', '\"', '.', \"'\", '\"', '\"', ',', ',', ',', ',', ',', ',', ',', '.', ',', '\"', '\"', '.', ',', ',', \"'\", ',', ',', ',', ',', ',', ',', '.', ',', '\"', '\"', ',', ',', '(', ')', '.', '\"', '\"', '.', ',', ',', ',', ',', ',', '(', ')', ',', '(', ')', ',', '(', ')', ',', ',', '.', '\"', '\"', ',', ',', ',', \"'\", ',', '.', ',', ',', ',', ',', ',', ',', ',', ',', '.', '\"', '\"', ',', ',', '.', ',', ',', ',', ',', '(', ')', '(', ')', ',', '(', ')', ',', ',', ',', '.', '.', '.', \"'\", '.', '(', ',', ')', ',', '.', ',', '.', ',', ',', '.', '.', '.', ',', ',', '.', ',', ',', ',', '.', '.', '.', '.', ',', '.', '.', '\"', '\"', ',', '.', ',', '.', ',', \"'\", '.', '.', ',', '.', '.', '.', ',', '.', '\"', '\"', ',', ',', ',', ',', '.', '.', ',', ',', '.', '.', '.', ',', ',', ',', ',', ',', '(', ')', ',', '.', '.', '.', ',', ',', '.', ',', ',', '.', ',', '.', ';', '\"', '\"', '.', \"'\", '.', '(', ')', ':', ',', ',', '(', ')', '.', ',', ';', '.', '.', '.', ',', ':', '.', '.', '.', '.', ',', ':', '(', ')', ';', '`', \"'\", '.', ',', ';', '`', \"'\", '.', '`', \"'\", '`', \"'\", '.', ',', \"'\", ';', ',', '\"', '\"', '.', '(', ')', ',', ',', '\"', '\"', ',', '.', ',', ',', ':', '.', '.', '.', '.', ',', '.', ',', '.', ',', ':', '.', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "load_corpus = load('corpus.txt')\n",
    "pattern = r'''(?x)\n",
    "    [][.,;\"'?!():_-`] # finds punctuation\n",
    "'''\n",
    "print(nltk.regexp_tokenize(load_corpus, pattern))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '29', '2007', '2007', 'Free Software Foundation', 'Preamble\\n\\n  The', 'General Public License', 'General Public License', 'Free Software Foundation', 'General Public License', 'Our General Public Licenses', '1', '2', '0', 'This License', '3', 'General Public License', 'The Program', 'Appropriate Legal Notices', '1', '2', '1', 'Source Code', 'Standard Interface', 'System Libraries', 'Major Component', 'Major\\nComponent', 'Major Component', 'Standard Interface', 'Major Component', 'Corresponding Source', 'System Libraries', 'Corresponding Source', 'The Corresponding Source', 'Corresponding\\nSource', 'The Corresponding Source', '2', 'Basic Permissions', 'This License', 'This License', '10', '3', 'Protecting Users', 'Legal Rights From Anti', 'Circumvention Law', '11', '20', '1996', '4', 'Conveying Verbatim Copies', '7', '5', 'Conveying Modified Source Versions', '4', '7', '4', 'This\\n    License', '7', 'This License', 'Appropriate Legal Notices', 'Appropriate Legal Notices', '6', 'Conveying Non', 'Source Forms', '4', '5', 'Corresponding Source', 'Corresponding Source', '1', 'Corresponding Source', '2', 'Corresponding Source', 'Corresponding Source', '6', 'Corresponding Source', 'Corresponding Source', 'Corresponding Source', 'Corresponding Source', 'Corresponding Source', 'Corresponding\\n    Source', '6', 'Corresponding Source', 'System Library', 'User Product', '1', '2', 'Installation Information', 'User Product', 'User Product', 'Corresponding Source', 'User Product', 'User Product', 'Corresponding Source', 'Installation Information', 'User Product', 'Installation Information', 'User Product', 'Corresponding Source', 'Installation Information', '7', 'Additional Terms', '15', '16', 'Appropriate Legal\\n    Notices', '10', '8', '11', '60', '30', '10', '9', 'Acceptance Not Required', 'Having Copies', '10', 'Automatic Licensing', 'Downstream Recipients', 'Corresponding Source', '11', 'Corresponding Source', '1', 'Corresponding Source', '2', '3', '28', '2007', '12', 'No Surrender', '13', 'Affero General Public License', '3', 'Affero General Public License', 'Affero General Public License', '13', '14', 'Revised Versions', 'The Free Software Foundation', 'General Public License', 'General\\nPublic License', 'Free Software\\nFoundation', 'General Public License', 'Free Software Foundation', 'General Public License', '15', '16', '17', '15', '16', 'Apply These Terms', 'Your New Programs\\n\\n  If', 'General Public License', 'Free Software Foundation', '3', 'E.  See', 'General Public License', 'General Public License', 'General Public License', 'General Public License', 'Lesser General\\nPublic License']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)\n",
    "\n",
    "          (?:[A-Z])(?:[a-z]+|\\.)(?:\\s+[A-Z](?:[a-z]+|\\.))*(?:\\s+[A-Z])(?:[a-z]+|\\.)\n",
    "                                         # proper names\n",
    "          | \\$\\d+\\s\\b[tr|b|m]illion\\b    # literal monetary amounts\n",
    "          | \\$?\\d+(?:[,\\.]\\d+)?          # numerical monetary amounts\n",
    "          | \\d{2}\\[\\\\]\\d{2}\\-\\\\]\\d{4}    # numerical dates (U.S. format)\n",
    "          | [A-Z][a-z.]*\\s\\d{2}\\,\\s\\d{2, 4} # literal dates (U.S. format)\n",
    "\n",
    "\n",
    "        '''\n",
    "\n",
    "print(nltk.regexp_tokenize(load_corpus, pattern))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-10\n",
    "# Rewrite the following loop as a list comprehension:\n",
    "# >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "# >>> result = []\n",
    "# >>> for word in sent:\n",
    "# ...     word_len = (word, len(word))\n",
    "# ...     result.append(word_len)\n",
    "# >>> result\n",
    "# [('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
    "\n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = [(word, len(word)) for word in sent]\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "['How much ', ' would a ', 'chuck chuck if a ', 'chuck could chuck ', '?']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-11\n",
    "# Define a string raw containing a sentence of your own choosing. Now, split raw on some character other than space,\n",
    "# such as 's'.\n",
    "\n",
    "raw = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "raw.split('wood')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "o\n",
      "m\n",
      "p\n",
      "a\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-12\n",
    "# Write a for loop to print out the characters of a string, one per line.\n",
    "\n",
    "string = \"Compared to some of the previous exercises, this seems comically easy.\"\n",
    "\n",
    "for s in string[:20]:\n",
    "    print(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', 'string', 'is', 'a', 'pretty', 'simple', 'string.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'strings', 'has', 'tabs.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstrings\\thas\\ttabs.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'lots', 'of', 'space.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This', '', '', '', '', '', '', '', 'string', '', '', '', '', '', '', '', '', '', 'has', '', '', '', '', '', 'lots', '', '', '', 'of', '', '', '', '', 'space.']\n",
      "\n",
      "With `sent.split()`:\n",
      "['This', 'string', 'has', 'tabs', 'and', 'spaces.']\n",
      "\n",
      "With `sent.split(' ')`:\n",
      "['This\\tstring', '', '', '', '', '', '', '', '', 'has\\ttabs', '', '', '', '', '', '', 'and\\tspaces.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-13\n",
    "# What is the difference between calling split on a string with no argument and one with ' ' as the argument, e.g.,\n",
    "# sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters,\n",
    "# consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter\n",
    "# a tab character.)\n",
    "\n",
    "s1 = \"This string is a pretty simple string.\"\n",
    "s2 = \"This\\tstrings\\thas\\ttabs.\"\n",
    "s3 = \"This        string          has      lots    of     space.\"\n",
    "s4 = \"This\\tstring         has\\ttabs       and\\tspaces.\"\n",
    "\n",
    "Ss = [s1, s2, s3, s4]\n",
    "\n",
    "for s in Ss:\n",
    "    print(\"\\nWith `sent.split()`:\")\n",
    "    print(s.split())\n",
    "    print(\"\\nWith `sent.split(' ')`:\")\n",
    "    print(s.split(' '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'woorden', 'words', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-14\n",
    "# Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is\n",
    "# the difference?\n",
    "\n",
    "words = [\"ord\", \"Wörter\", \"words\", \"palabras\", \"sanat\",\n",
    "         \"mots\", \"focail\", \"parole\", \"words\", \"woorden\",\n",
    "         \"palavras\"]\n",
    "\n",
    "words.sort()\n",
    "print(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'woorden', 'words', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wörter', 'focail', 'mots', 'ord', 'palabras', 'palavras', 'parole', 'sanat', 'woorden', 'words', 'words']\n"
     ]
    }
   ],
   "source": [
    "words = [\"ord\", \"Wörter\", \"words\", \"palabras\", \"sanat\",\n",
    "         \"mots\", \"focail\", \"parole\", \"words\", \"woorden\",\n",
    "         \"palavras\"]\n",
    "\n",
    "print(sorted(words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ord', 'Wörter', 'words', 'palabras', 'sanat', 'mots', 'focail', 'parole', 'words', 'woorden', 'palavras']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "'3333333'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-15\n",
    "# Explore the difference between strings and integers by typing the following at a Python prompt: \"3\" * 7 and 3 * 7.\n",
    "# Try converting between strings and integers using int(\"3\") and str(3).\n",
    "\n",
    "\"3\" * 7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "21"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "21"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\") * 7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'3333333'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(3) * 7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monty' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-33-760651dede18>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# evaluate the expression test.monty at the prompt.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mmonty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'monty' is not defined"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-16\n",
    "# Earlier, we asked you to use a text editor to create a file called test.py, containing the single line\n",
    "# monty = 'Monty Python'. If you haven’t already done this (or can’t find the file), go ahead and do it now.\n",
    "# Next, start up a new session with the Python interpreter, and enter the expression monty at the prompt.\n",
    "# You will get an error from the interpreter. Now, try the following (note that you have to leave off the .py\n",
    "# part of the filename):\n",
    "# >>> from test import msg\n",
    "# >>> msg\n",
    "# This time, Python should return with a value. You can also try import test, in which case Python should be able to\n",
    "# evaluate the expression test.monty at the prompt.\n",
    "\n",
    "monty\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from test import monty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'Monty Python'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "'Monty Python'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.monty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "'another test'"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-17\n",
    "# What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six\n",
    "# characters?\n",
    "\n",
    "test = \"another test\"\n",
    "\n",
    "\"%6s\" % (test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "'hey   '"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%-6s\" % (\"hey\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "'anothe'"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%.6s\" % (test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Exercise: 3-18\n",
    "# Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in\n",
    "# English are used in questions, relative clauses, and exclamations: who, which, what, and so on.) Print them in\n",
    "# order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "raw = gutenberg.raw('bryant-stories.txt')\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "tokens = sorted(set(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whale', 'What', 'When', 'Whenever', 'Where', 'Whether', 'Whiff', 'While', 'Whirling', 'White', 'Who', 'Whose', 'Why', 'what', 'whatever', 'wheat', 'wheelbarrow', 'wheeled', 'when', 'whence', 'whenever', 'where', 'wherein', 'wherever', 'whether', 'which', 'while', 'whimpering', 'whin', 'whinny', 'whipped', 'whirlpool', 'whiruled', 'whisk', 'whisked', 'whisper', 'whisper_', 'whispered', 'whispering', 'whispers', 'whistle', 'whistled', 'white', 'white-haired', 'white-robed', 'whither', 'who', 'whole', 'wholly', 'whom', 'whose', 'why']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in tokens if re.search('^[Ww]h', w)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Exercise: 3-19\n",
    "# Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space\n",
    "# character, and a positive integer, e.g., fuzzy 53. Read the file into a Python list using\n",
    "# open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an\n",
    "# integer using int(). The result should be a list of the form: [['fuzzy', 53], ...].\n",
    "\n",
    "fuzzy = open('fuzzy.txt', encoding = \"utf-8\").readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "[['fuzzy', 53], ['puppy', 7], ['hubby', 18]]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[word, int(value)] for word, value in (f.split() for f in fuzzy)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Exercise: 3-20\n",
    "# Write code to access a favorite web page and extract some text from it. For example, access a weather site and\n",
    "# extract the forecast top temperature for your town or city today.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "key = 'YOUR KEY'\n",
    "\n",
    "def k_to_c(temp):\n",
    "    \"\"\"Converts Kelvin to Celsius.\"\"\"\n",
    "    return temp - 273.15\n",
    "\n",
    "def k_to_f(temp):\n",
    "    \"\"\"Converts Kelvin to Fahrenheit.\"\"\"\n",
    "    return (temp - 273.15) * 1.8 + 32\n",
    "\n",
    "def get_temp(city):\n",
    "    \"\"\"\n",
    "    Gets current, high, and low temperatures for a given city.\n",
    "    \"\"\"\n",
    "    r = requests.get('http://api.openweathermap.org/data/2.5/weather?q=' + city + '&APPID=' + key)\n",
    "\n",
    "    if r.json()['cod'] == '404':\n",
    "        print(\"Sorry, we don't know where that city is.\")\n",
    "    else:\n",
    "        current_k = r.json()['main']['temp']\n",
    "        min_k = r.json()['main']['temp_min']\n",
    "        max_k = r.json()['main']['temp_max']\n",
    "        print(\"The current temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(current_k), k_to_f(current_k)))\n",
    "        print(\"Today's high temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(max_k), k_to_f(max_k)))\n",
    "        print(\"Today's low temperature is {:.1f} C°/{:.1f} F°.\".format(k_to_c(min_k), k_to_f(min_k)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'main'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-46-de790c5bf3bf>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mget_temp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Odense'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-45-9e75359fddd9>\u001B[0m in \u001B[0;36mget_temp\u001B[0;34m(city)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Sorry, we don't know where that city is.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m         \u001B[0mcurrent_k\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'main'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'temp'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0mmin_k\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'main'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'temp_min'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0mmax_k\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'main'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'temp_max'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'main'"
     ]
    }
   ],
   "source": [
    "get_temp('Odense')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Exercise: 3-21\n",
    "# Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that\n",
    "# web page. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and\n",
    "# remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words\n",
    "# manually and discuss your findings.\n",
    "\n",
    "wordlist = [w.lower() for w in nltk.corpus.words.words('en')]\n",
    "\n",
    "# irregular verbs\n",
    "verbs = ['ate', 'beat', 'beaten', 'became', 'become', 'began', 'begun', 'bent',\n",
    "         'bet', 'bid', 'bit', 'bitten', 'blew', 'blown', 'bought', 'broke', 'broken',\n",
    "         'brought', 'built', 'burnt', 'came', 'caught', 'chose', 'chosen', 'come',\n",
    "         'cost', 'cut', 'did', 'dived', 'done', 'dove', 'drank', 'drawn', 'dreamt',\n",
    "         'drew', 'driven', 'drove', 'drunk', 'dug', 'eaten', 'fallen', 'fell', 'felt',\n",
    "         'flew', 'flown', 'forgave', 'forgiven', 'forgot', 'forgotten', 'fought', 'found',\n",
    "         'froze', 'frozen', 'gave', 'given', 'gone', 'got', 'gotten', 'grew', 'grown',\n",
    "         'had', 'heard', 'held', 'hid', 'hidden', 'hit', 'hung', 'hurt', 'kept', 'knew',\n",
    "         'known', 'laid', 'lain', 'lay', 'led', 'left', 'lent', 'let', 'lost', 'made',\n",
    "         'meant', 'met', 'paid', 'put', 'ran', 'rang', 'read', 'ridden', 'risen', 'rode',\n",
    "         'rose', 'run', 'rung', 'said', 'sang', 'sat', 'saw', 'seen', 'sent', 'showed',\n",
    "         'shown', 'shut', 'slept', 'sold', 'spent', 'spoke', 'spoken', 'stood', 'sung',\n",
    "         'swam', 'swum', 'taken', 'taught', 'thought', 'threw', 'thrown', 'told', 'took',\n",
    "         'tore', 'torn', 'understood', 'went', 'woke', 'woken', 'won', 'wore', 'worn',\n",
    "         'written', 'wrote']\n",
    "\n",
    "wordlist += verbs\n",
    "\n",
    "def unknown(url, es = False, s = False, ed = False, ing = False, n = False, er = False):\n",
    "\n",
    "    # get text\n",
    "    raw = return_URL_contents(url)\n",
    "\n",
    "    # get lower-case words\n",
    "    raw_lower = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "\n",
    "    # find unknown words and eliminate duplicates\n",
    "    unknown = sorted(set([w for w in raw_lower if w not in wordlist]))\n",
    "\n",
    "    # find common words that are not in wordlist because of\n",
    "    # morphological changes\n",
    "    exclude = []\n",
    "\n",
    "    # words with -es plurals\n",
    "    if es:\n",
    "        es = [i for i in unknown if i[-2:] == 'es' and i[:-2] in wordlist]\n",
    "        exclude += es\n",
    "        # -y becomes -ies\n",
    "        ies = [i for i in unknown if i[-3:] == 'ies' and i[:-3] + 'y' in wordlist]\n",
    "        exclude += ies\n",
    "\n",
    "    # regular plurals\n",
    "    if s:\n",
    "        s = [i for i in unknown if i[-1] == 's' and i[:-1] in wordlist]\n",
    "        exclude += s\n",
    "\n",
    "    # regular past tense forms\n",
    "    if ed:\n",
    "        # verbs with final -e\n",
    "        d = [i for i in unknown if i[-1:] == 'd' and i[:-1] in wordlist]\n",
    "        exclude += d\n",
    "        # regular verbs\n",
    "        ed = [i for i in unknown if i[-2:] == 'ed' and i[:-2] in wordlist]\n",
    "        exclude += ed\n",
    "        # verbs that double final consonant\n",
    "        dd = [i for i in unknown if i[-2:] == 'ed' and i[:-3] in wordlist]\n",
    "        exclude += dd\n",
    "\n",
    "    # regular gerunds\n",
    "    if ing:\n",
    "        # verbs with final -e\n",
    "        ng = [i for i in unknown if i[-3:] == 'ing' and i[:-3] + 'e' in wordlist]\n",
    "        exclude += ng\n",
    "        # regular verbs\n",
    "        ing = [i for i in unknown if i[-3:] == 'ing' and i[:-3] in wordlist]\n",
    "        exclude += ing\n",
    "        # verbs that double final consonat\n",
    "        nng = [i for i in unknown if i[-3:] == 'ing' and i[:-4] in wordlist]\n",
    "        exclude += nng\n",
    "\n",
    "    if n:\n",
    "        # negative contractions without final -'t\n",
    "        n = [i for i in unknown if i[-1:] == 'n' and i[:-1] in wordlist]\n",
    "        exclude += n\n",
    "\n",
    "    if er:\n",
    "        # comparative forms\n",
    "        er = [i for i in unknown if i[-2:] == 'er' and i[:-2] in wordlist]\n",
    "        exclude += er\n",
    "        # comparative forms with final -y\n",
    "        ier = [i for i in unknown if i[-3:] == 'ier' and i[:-3] + 'y' in wordlist]\n",
    "        exclude += ier\n",
    "        # comparative forms with final -e\n",
    "        r = [i for i in unknown if i[-2:] == 'er' and i[:-1] in wordlist]\n",
    "        exclude += r\n",
    "        # superlative forms\n",
    "        est = [i for i in unknown if i[-3:] == 'est' and i[:-3] in wordlist]\n",
    "        exclude += est\n",
    "        # superlative forms with final -y\n",
    "        st = [i for i in unknown if i[-3:] == 'est' and i[:-2] in wordlist]\n",
    "        exclude += st\n",
    "        # superlative forms with final -e\n",
    "        iest = [i for i in unknown if i[-4:] == 'iest' and i[:-4] + 'y' in wordlist]\n",
    "        exclude += iest\n",
    "\n",
    "    # return only those unknown words that have not been excluded\n",
    "    # by the above list comprehensions\n",
    "    return [i for i in unknown if i not in exclude]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ads', 'agencies', 'answers', 'attacking', 'attempts', 'audiences', 'banned', 'billionaires', 'bills', 'bringing', 'businesses', 'called', 'candidates', 'changing', 'cheaper', 'companies', 'comparing', 'competitors', 'contributions', 'cookies', 'coronavirus', 'couldn', 'criticized', 'debated', 'diagnosing', 'didn', 'discovers', 'discussed', 'doors', 'enemies', 'enjoyed', 'executives', 'explained', 'explodes', 'frontrunner', 'fundraising', 'giants', 'harshest', 'has', 'ignoring', 'including', 'indicated', 'infused', 'laws', 'logo', 'looks', 'mainstream', 'masterclass', 'means', 'members', 'minutes', 'monopolies', 'offered', 'options', 'personalized', 'podcasts', 'politicians', 'practices', 'pressed', 'proposals', 'pushed', 'readers', 'recognizing', 'regulators', 'represents', 'required', 'rights', 'rules', 'schools', 'sectors', 'sees', 'sharing', 'shifted', 'showcased', 'signing', 'simmering', 'specifics', 'stories', 'techlash', 'technologies', 'teddyschleifer', 'themes', 'topics', 'trackers', 'tracking', 'users', 'using', 'wants', 'whacks', 'years']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.vox.com/recode/2019/10/16/20916712/cnn-democratic-presidential-debate-big-tech-silicon-valley-warren-harris'\n",
    "\n",
    "print(unknown(url))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronavirus', 'frontrunner', 'fundraising', 'logo', 'mainstream', 'masterclass', 'podcasts', 'techlash', 'teddyschleifer']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.vox.com/recode/2019/10/16/20916712/cnn-democratic-presidential-debate-big-tech-silicon-valley-warren-harris'\n",
    "\n",
    "print(unknown(url, es = True, s = True, ed = True, ing = True, n = True, er = True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "'Home - BBC News Homepage Accessibility links Skip to content Accessibility Help BBC Account Notifications Home News Sport Weather iPlayer Sounds CBBC CBeebies Food Bitesize Arts Taster Local Three Menu Search Search the BBC Search the BBC BBC News News Navigation Sections Home Home selected Coronavirus Video World UK Business Tech Science Stories Entertainment & Arts Health World News TV In Pictures Reality Check Newsbeat Long Reads More More sections Home Home selected Coronavirus Coronavirus Home Your Coronavirus Stories Video World World Home Africa Asia Australia Europe Latin America Middle East US & Canada UK UK Home England N. Ireland Scotland Wales Politics Local News Business Business Home Market Data New Economy New Tech Economy Companies Entrepreneurship Technology of Business Global Education Economy Global Car Industry Business of Sport Tech Science Stories Entertainment & Arts Health Health Home Coronavirus World News TV In Pictures Reality Check Newsbeat Long Reads BBC News Home Breaking Breaking news Close breaking news Latest Stories Most Read Skip to most read Latest Stories Most Read Top Stories Patients die without oxygen amid Delhi Covid surge Delhi hospitals run out of beds and oxygen as families struggle to save their loved ones. 3h 3 hours ago Asia Patients die without oxygen amid Delhi Covid surge Delhi hospitals run out of beds and oxygen as families struggle to save their loved ones. 3h 3 hours ago Asia Related content Indian hospitals send SOS as Covid toll surges Video Inside the Delhi hospital at breaking point Deaths climb as India reels from deadly Covid wave Dozens dead after Baghdad hospital fire Flames across several floors lead to scenes of panic at a hospital in the Iraqi capital. 17m 18 minutes ago Middle East Biden says Armenian mass killing was genocide The statement by the US president drew an immediate rebuke from Turkey, which disputes the term. 12h 12 hours ago US & Canada Video 2 minutes 57 seconds Video 2 minutes 57 secon'"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-22\n",
    "# Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested above.\n",
    "# You will see that there is still a fair amount of non-textual data there, particularly JavaScript commands. You\n",
    "# may also find that sentence breaks have not been properly preserved. Define further regular expressions that\n",
    "# improve the extraction of text from this web page.\n",
    "\n",
    "url = \"https://www.bbc.com/news\"\n",
    "return_URL_contents(url)[:2000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "[('do', \"n't\")]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-23\n",
    "# Are you able to write a regular expression to tokenize text in such a way that the word don’t is tokenized into do\n",
    "# and n’t? Explain why this regular expression won’t work: «n't|\\w+».\n",
    "\n",
    "re.findall(r\"(.*?)(n't)|\\w+\", \"don't\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "'h3||0 5uck3r55w33t! 1 8 y0ur |unch5w33t! 1t wa5 d3|15h5w33t!'"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-24\n",
    "# Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1,\n",
    "# o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more\n",
    "# substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s.\n",
    "\n",
    "test = \"Hello suckers. I ate your lunch. It was delish.\"\n",
    "\n",
    "test = test.lower()\n",
    "\n",
    "org = ['ate', 'e', 'i', 'o', 'l', 's', '\\.']\n",
    "sub = ['8', '3', '1', '0', '|', '5', '5w33t!']\n",
    "\n",
    "for i in range(len(org)):\n",
    "    test = re.sub(org[i], sub[i], test)\n",
    "\n",
    "test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Exercise: 3-25\n",
    "# Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any\n",
    "# consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g., string →\n",
    "# ingstray, idle → idleay (see http://en.wikipedia.org/wiki/Pig_Latin).\n",
    "# a: Write a function to convert a word to Pig Latin.\n",
    "# b: Write code that converts text, instead of individual words.\n",
    "# c: Extend it further to preserve capitalization, to keep qu together (so that quiet becomes ietquay, for example),\n",
    "# and to detect when y is used as a consonant (e.g., yellow) versus a vowel (e.g., style).\n",
    "\n",
    "def pig_latin(word):\n",
    "    \"\"\"\n",
    "    Returns pig latin version of word.\n",
    "    \"\"\"\n",
    "\n",
    "    # replace 'dumb' quotes\n",
    "    word = re.sub(\"’\", \"'\", word)\n",
    "\n",
    "    # won't work on non-alphabetic strings\n",
    "    if not word.isalpha():\n",
    "        if \"'\" not in word:\n",
    "            return word\n",
    "\n",
    "    # Return uppercase word if original is in uppercase\n",
    "    caps = False\n",
    "    if word[0].isupper():\n",
    "        caps = True\n",
    "    word = word.lower()\n",
    "\n",
    "    # word starts with vowel\n",
    "    if word[0] in 'AEIOUaeiou':\n",
    "        pl = word + 'ay'\n",
    "\n",
    "    # some tokenizers will produce non-words\n",
    "    elif len(word) == 1:\n",
    "        return word\n",
    "\n",
    "    # word begins with 'y' - treated as a consonant\n",
    "    # otherwise 'y' is a vowel, or first vowel is not 'y'\n",
    "    elif word[0] == 'y':\n",
    "        pl = word[1:] + 'yay'\n",
    "\n",
    "    # word begins with 'qu'\n",
    "    elif word[:2] == \"qu\":\n",
    "        pl = word[2:] + \"quay\"\n",
    "\n",
    "    # all other cases\n",
    "    else:\n",
    "        start, end = re.findall(r'\\b^[^aeiouy]*|[aeiouy]{1}\\S*', word)\n",
    "        pl = end + start + 'ay'\n",
    "\n",
    "    # restore word to uppercase if necessary\n",
    "    if caps:\n",
    "        pl = pl[0].upper() + pl[1:]\n",
    "    return pl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def join_punctuation(text, characters = [\"'\", '’', ')', ',', '.', ':', ';', '?', '!', ']', \"''\"]):\n",
    "    \"\"\"\n",
    "    Takes a list of strings and attaches punctuation to\n",
    "    the preceding string in the list.\n",
    "    \"\"\"\n",
    "\n",
    "    text = iter(text)\n",
    "    current = next(text)\n",
    "\n",
    "    for nxt in text:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "\n",
    "\n",
    "    yield current"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def pig_latin_text(text):\n",
    "    \"\"\"\n",
    "    Translates text into pig latin.\n",
    "    \"\"\"\n",
    "    pl = []\n",
    "    for t in re.findall(r'\\b[\\S]+\\b|[.,!?]', text):\n",
    "        pl.append(pig_latin(t))\n",
    "\n",
    "    return \" \".join(join_punctuation(pl))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Oneay imetay inay ixthsay adegray eway ereway atay ecessray anday ilewhay Iay asway unningray otay ymay iendsfray, Iay ustjay osay appenedhay otay ickkay aay Ugehay ockray anday ithoutway inkingthay Iay outedshay atay ethay optay ofay ymay ungslay Otherfuckermay Anday ithway ymay god-awful ucklay, ymay athmay eachertay asway ittingsay atay ethay enchbay ightray Esidebay Emay. Ehay enthay ooktay emay insideay otay atwhay Iay oughtthay asway ellyay atay emay utbay ehay ustjay ouldn'tcay opstay aughinglay anday entsay emay ackbay outsideay ithway aay iterallay andycay arbay. Ehay isay illstay ymay avoritefay eachertay I'veay everay adhay.\""
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = \"\"\"One time in sixth grade we were at recess and while I was running to\n",
    "my friends, I just so happened to kick a HUGE rock and without thinking I\n",
    "shouted at the top of my lungs MOTHERFUCKER And with my god-awful luck, my math\n",
    "teacher was sitting at the bench right BESIDE ME. He then took me inside to\n",
    "what I thought was yell at me but he just couldn’t stop laughing and sent\n",
    "me back outside with a literal candy bar. He is still my favorite teacher\n",
    "I've ever had.\"\"\"\n",
    "\n",
    "pig_latin_text(story)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# Exercise: 3-26\n",
    "# Download some text from a language that has vowel harmony (e.g., Hungarian), extract the vowel sequences of words,\n",
    "# and create a vowel bigram table.\n",
    "\n",
    "def clean_pg_text(text, encode = \"utf8\"):\n",
    "    \"\"\"\n",
    "    Returns a list of words from a Project Gutenberg text.\n",
    "    Headers and footers are removed from texts.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    text: name of file\n",
    "    encode: text encoding used in file. Default is UTF-8\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    opened_text = open(text, 'r', encoding = encode)\n",
    "    cleaned_text = []\n",
    "    flag = False\n",
    "    start = \"*** START OF\"\n",
    "    end = \"*** END OF\"\n",
    "\n",
    "    # some PG texts don't use spaces to designate start/end of text\n",
    "    alt_start = \"***START OF\"\n",
    "    alt_end = \"***END OF\"\n",
    "\n",
    "    for line in opened_text:\n",
    "\n",
    "        # start reading in lines after boilerplate\n",
    "        if ((start in line) or (alt_start in line)) and flag == False:\n",
    "            flag = True\n",
    "\n",
    "        # return word list once boilerplate has been reached\n",
    "        elif ((end in line) or (alt_end in line)) and flag == True:\n",
    "            return cleaned_text\n",
    "        elif flag:\n",
    "\n",
    "                for word in line.split():\n",
    "                    word = word.strip().lower()\n",
    "                    cleaned_text.append(word)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "raw = clean_pg_text('hungarian.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['produced', 'by', 'albert', 'lászló', 'from', 'page', 'images', 'generously', 'made', 'available', 'by', 'the', 'google', 'books', 'library', 'project', 'jókai', 'mór', 'összes', 'művei', 'nemzeti', 'kiadás', 'xxx.', 'kötet', 'a', 'kőszivű', 'ember', 'fiai.', 'ii.', 'budapest', 'révai', 'testvérek', 'kiadása', '1895', 'a', 'kőszivű', 'ember', 'fiai', 'regény', 'irta', 'jókai', 'mór', 'ii.', 'rész', 'a', 'franklin-társulat', 'tulajdona', 'budapest', 'révai', 'testvérek', 'kiadása', '1895', 'egy', 'nemzeti', 'hadsereg.', 'mesemondás!', 'lehetett', 'is', 'az', 'valaha!', 'hogy', 'egy', 'kicsiny,', 'elszigetelt', 'országnak', 'rokontalan', 'nemzete', 'valaha', 'saját', 'haderejével,', 'kilencz', 'oldalról', 'rárohanó', 'támadás', 'ellen', 'védelmezte', 'volna', 'magát,', 'diadallal,', 'dicsőséggel!', 'hogy', 'ne', 'birt', 'volna', 'vele', '«egy»', 'óriás!', 'hogy', 'rá', 'kellett', 'volna', 'ereszteni', 'európa', 'másik', 'koloszszát', 'is,', 's', 'még', 'azzal', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(raw[:100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'zöld', 'füvön,', 'az', 'arany', 'verőfényen', 'fehér', 'báránykával', 'játszik', 'mosolygó', 'arczú', 'kis', 'gyermek,', '–', 's', 'kék', 'és', 'fehér', 'mezővirágokból', 'koszorut', 'fon', 'a', 'fehér', 'bárányka', 'nyakába.', '…', 'jó', 'az', 'isten!', '–', 'zöld', 'fűvet', 'ád', 'a', 'harczáztatta', 'földnek;', 'fehér', 'bárányt', 'a', 'zöld', 'fűnek;', 'ártatlan', 'szelid', 'angyalkát', 'játszótársúl', 'a', 'báránynak;', '–', 'feledés', 'írját', 'a', 'mély', 'sebnek;', '–', 'jobb', 'idők', 'reményét', 'a', 'szegény', 'magyar', 'nemzetnek.', 'tartalom.', 'ii.', 'kötet.', 'egy', 'nemzeti', 'hadsereg', '1', 'a', 'szalmakomisszárius', '6', 'az', 'első', 'tandíj', '27', 'a', 'betyár', '35', 'a', 'királyerdőben', '50', 'a', 'haldokló', 'ellenfél', 'hagyatéka', '58', 'napfény', 'és', 'holdfény', '66', 'sötétség', '79', 'mindenváró', 'ádám', '88', 'sorsát', 'senki', 'sem', 'kerülheti', 'el', '107', 'egy', 'magános', 'lovag', '124', 'várharcz', 'mennykövekkel', '132', 'zenith', '157', 'az', 'eldobott', 'lélek', '168', 'ephialtes', '179', 'perhelia', '190', 'régi', 'jó', 'barátok', '194', 'nadir', '203', 'a', 'nem', 'mutatott', 'levél', '218', 'egy', 'ember', 'a', 'kit', 'még', 'eddig', 'nem', 'ismertünk', '224', 'a', 'túlvilágról', '230', 'a', 'kőszivű', 'ember', 'előtt', '237', 'a', 'börtön', 'távirdája', '239', 'az', 'első', 'tőrdöfés', '240', 'a', 'fejgörcsök', 'napján', '241', 'a', 'tőr', 'hegye', 'letörve', '248', 'a', 'kőszivű', 'ember', 'felel', '253', 'a', 'kérő', '254', 'comedie', 'of', 'errors', '262', 'a', 'szenvedések', 'kulcsa', '266', 'húsz', 'év', 'mulva', '274', 'végszó', '284', 'franklin-társulat', 'nyomdája.', 'end', 'of', 'project', \"gutenberg's\", 'a', 'koszivu', 'ember', 'fiai', '(2.', 'rész),', 'by', 'mór', 'jókai']\n"
     ]
    }
   ],
   "source": [
    "print(raw[-200:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "72569"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = raw[16:-158]\n",
    "len(raw)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "hv = []\n",
    "for hw in raw:\n",
    "    hv.append(re.findall(r'([aeiouáéíóöúüőű])', hw))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "hv_pairs = []\n",
    "\n",
    "for h in hv:\n",
    "    if len(h) == 2:\n",
    "        hv_pairs.append(h[0] + h[1])\n",
    "    elif len(h) > 2:\n",
    "        hv_pairs.append(h[-2] + h[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "42867"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hv_pairs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    e    i    o    u    á    é    í    ó    ö    ú    ü    ő    ű \n",
      "a 2985  253 1157 1691  237 1313  172    8  381   11   39    3    3    0 \n",
      "e  139 5443 1906  119   40  291 1483   11    7   48    7  456  811   74 \n",
      "i  807 1471  172  483  145  314  247    2   99    6   14   19  159   12 \n",
      "o 2640   39  298  824  149  769   74   10  346    1   76    0    2    2 \n",
      "u  644   28  102  472   25  367   11    0   88    1    1    0    1    0 \n",
      "á 2209   25  465  823  135  523   75   15  323    3   42    1    2    2 \n",
      "é   85 2253  390   62    7   63  231    0   16   16    1   79  234   30 \n",
      "í  133  108   45   71    1   86   28    0   25    0    0   14   13   16 \n",
      "ó  307    5   49   60   17   60   10    2   23    1    3    0    7    0 \n",
      "ö    4  655  164    0    0    7  272    2    0  514    0  137   74   32 \n",
      "ú  160    6   41   63    3   34    3    1   22   15    1    0    8    0 \n",
      "ü    2  263   70    0    0    0   76    0    0  103    0   15   42    1 \n",
      "ő    6  464   77    4    0    1   52    1    0   83    3   43   23    1 \n",
      "ű    1   67   15    1    0    1   16    0    0    9    0    1    1    5 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(hv_pairs)\n",
    "cfd.tabulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ua', 'úú', 'io', 'óo', 'úó', 'áo', 'au', 'íó', 'ái', 'iá', 'íú', 'ou', 'ói', 'áá', 'ia', 'óá', 'áa', 'uí', 'óa', 'úo', 'úi', 'ío', 'úá', 'íá', 'úa', 'uu', 'óí', 'óú', 'áí', 'ía', 'oó', 'oú', 'aó', 'aú', 'iu', 'úí', 'aí', 'áu', 'óu', 'oi', 'oo', 'ao', 'úu', 'ai', 'oá', 'uó', 'aá', 'uú', 'oa', 'aa', 'íu', 'óó', 'uo', 'iú', 'ui', 'áú', 'ió', 'oí', 'áó', 'uá']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# back vowels\n",
    "bv = ['a', 'á', 'i', 'í', 'o', 'ó', 'u', 'ú']\n",
    "\n",
    "# create power set\n",
    "bvps = set(list(itertools.product(bv, bv)))\n",
    "\n",
    "# convert items in power set to strings\n",
    "bvpj = [b[0] + b[1] for b in bvps]\n",
    "\n",
    "# Remove combinations with only i or í\n",
    "tbd = ['ií', 'íi', 'ii', 'íí']\n",
    "for i in tbd:\n",
    "    bvpj.remove(i)\n",
    "\n",
    "print(bvpj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "22181"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_vowels = 0\n",
    "for i in bvpj:\n",
    "    back_vowels += hv_pairs.count(i)\n",
    "\n",
    "back_vowels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['éö', 'eü', 'űő', 'ée', 'öü', 'íö', 'üő', 'űé', 'őű', 'íe', 'eí', 'üé', 'öí', 'iü', 'őö', 'űű', 'éi', 'üű', 'eő', 'őe', 'űö', 'eé', 'éü', 'öő', 'üö', 'öé', 'űe', 'íü', 'ői', 'eű', 'üe', 'éí', 'iő', 'öű', 'ié', 'űi', 'őü', 'eö', 'üi', 'éő', 'öö', 'íő', 'ee', 'iű', 'öe', 'éé', 'őí', 'űü', 'üü', 'íé', 'iö', 'ei', 'éű', 'ie', 'öi', 'íű', 'őő', 'űí', 'üí', 'őé']\n"
     ]
    }
   ],
   "source": [
    "fv = ['e', 'é', 'i', 'í', 'ö', 'ő', 'ü', 'ű']\n",
    "fvps = set(list(itertools.product(fv, fv)))\n",
    "fvpj = [f[0] + f[1] for f in fvps]\n",
    "\n",
    "tbd = ['ií', 'íi', 'ii', 'íí']\n",
    "\n",
    "for i in tbd:\n",
    "    fvpj.remove(i)\n",
    "\n",
    "print(fvpj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "18836"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "front_vowels = 0\n",
    "for i in fvpj:\n",
    "    front_vowels += hv_pairs.count(i)\n",
    "\n",
    "front_vowels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "4.315674061632492"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(hv_pairs) - front_vowels - back_vowels)/len(hv_pairs) * 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "' a ah heeee a haah aeahaeehheeaea ha heaeaeeahhe e e    aehh  aeeeea heaehhhhhh  ehhe hh ehahae heheaheeehaaeh hhe  ea a ehe eehaah   h hh hhaeeaaah   hahaahe hh eeeheehha e e hhhhheh hhhhee heh hh  heheahah ehha h hahh hehee hh ehe hh ehhee  ahe heeeahha ae aeheaeh  ehehahh a ahh  ehehehaaeha aha hh ahahhehhh hah e  hheee ahheheah heh he hehhheeaehhh ehhhhha hhe eeaeheaheah hh haeheheh hahhha ehaaehhaeahh haha a aa hhh e heehhah aa  hh hhhahehhe  ahhhhh hehehehh hhe  ee   h   aa   ahhhe hea h h'"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-27\n",
    "# Python’s random module includes a function choice() which randomly chooses an item from a sequence; e.g.,\n",
    "# choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the\n",
    "# others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the\n",
    "# string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one\n",
    "# long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he haha ee\n",
    "# heheeh eha. Use split() and join() again to normalize the whitespace in this string.\n",
    "\n",
    "import random\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for i in range(500):\n",
    "    new_list.append(random.choice(\"aehh \"))\n",
    "\n",
    "string = ''.join(new_list)\n",
    "string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ah heeee a haah aeahaeehheeaea ha heaeaeeahhe e e aehh aeeeea heaehhhhhh ehhe hh ehahae heheaheeehaaeh hhe ea a ehe eehaah h hh hhaeeaaah hahaahe hh eeeheehha e e hhhhheh hhhhee heh hh heheahah ehha h hahh hehee hh ehe hh ehhee ahe heeeahha ae aeheaeh ehehahh a ahh ehehehaaeha aha hh ahahhehhh hah e hheee ahheheah heh he hehhheeaehhh ehhhhha hhe eeaeheaheah hh haeheheh hahhha ehaaehhaeahh haha a aa hhh e heehhah aa hh hhhahehhe ahhhhh hehehehh hhe ee h aa ahhhe hea h h\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(string.split()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# Exercise: 3-29\n",
    "# Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of\n",
    "# appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs\n",
    "# to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text\n",
    "# is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus,\n",
    "# including section f (popular lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces\n",
    "# a sequence of words, whereas nltk.corpus.brown.sents() produces a sequence of sentences.\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "def get_brown_ari(cat):\n",
    "    \"\"\"\n",
    "    Returns the Automated Readability Index (ARI) of a given\n",
    "    category of the Brown Corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate total letters in the category\n",
    "    total_letters = 0\n",
    "    for w in brown.words(categories = cat):\n",
    "        total_letters += len(w)\n",
    "\n",
    "    # calculate average number of letters per word\n",
    "    mu_w = total_letters/len(brown.words(categories = cat))\n",
    "\n",
    "    # calculate average number of words per sentence\n",
    "    mu_s = len(brown.words(categories = cat)) / len(brown.sents(categories = cat))\n",
    "\n",
    "    return (4.71 * mu_w) + (0.5 * mu_s) - 21.43"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Brown Corpus, the Automated Readability Index (ARI) for the category:\n",
      "\n",
      "                                             ...\"Adventure\" is 4.0842.\n",
      "                                             ...\"Belles Lettres\" is 10.9877.\n",
      "                                             ...\"Editorial\" is 9.4710.\n",
      "                                             ...\"Fiction\" is 4.9105.\n",
      "                                             ...\"Government\" is 12.0843.\n",
      "                                             ...\"Hobbies\" is 8.9224.\n",
      "                                             ...\"Humor\" is 7.8878.\n",
      "                                             ...\"Learned\" is 11.9260.\n",
      "                                             ...\"Lore\" is 10.2548.\n",
      "                                             ...\"Mystery\" is 3.8336.\n",
      "                                             ...\"News\" is 10.1767.\n",
      "                                             ...\"Religion\" is 10.2031.\n",
      "                                             ...\"Reviews\" is 10.7697.\n",
      "                                             ...\"Romance\" is 4.3492.\n",
      "                                             ...\"Science Fiction\" is 4.9781.\n"
     ]
    }
   ],
   "source": [
    "print(\"In the Brown Corpus, the Automated Readability Index (ARI) for the category:\\n\")\n",
    "\n",
    "for c in brown.categories():\n",
    "    ari = get_brown_ari(c)\n",
    "    c = re.sub('_', ' ', c)\n",
    "    print(\"{:45}...\\\"{}\\\" is {:.4f}.\".format(\"\",c.title(), ari) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Porter - Wikipedia Martin Porter From Wikipedia, the free encyclopedia Jump to navigation Jump to search For the musician, see Martin Porter (musician) . Martin F. Porter is the inventor of the Porter Stemmer , [1] one of the most common algor\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-30\n",
    "# Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with\n",
    "# the Lancaster Stemmer, and see if you observe any differences.\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Martin_Porter'\n",
    "\n",
    "to_be_stemmed = return_URL_contents(url)\n",
    "print(to_be_stemmed[:250])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Martin', 'Porter', '-', 'Wikipedia', 'Martin', 'Porter', 'From', 'Wikipedia', ',', 'the', 'free', 'encyclopedia', 'Jump', 'to', 'navigation', 'Jump', 'to', 'search', 'For', 'the', 'musician', ',', 'see', 'Martin', 'Porter', '(', 'musician', ')', '.', 'Martin', 'F.', 'Porter', 'is', 'the', 'inventor', 'of', 'the', 'Porter', 'Stemmer', ',', '[', '1', ']', 'one', 'of', 'the', 'most', 'common', 'algorithms', 'for', 'stemming', 'English', ',', '[', '2', ']', '[', '3', ']', 'and', 'the', 'Snowball', 'programming', 'framework', '.', 'His', '1980', 'paper', '``', 'An', 'algorithm', 'for', 'suffix', 'stripping', \"''\", ',', 'proposing', 'the', 'stemming', 'algorithm', ',', 'has', 'been', 'cited', 'over', '8000', 'times', '(', 'Google', 'Scholar', ')', '.', '[', '4', ']', 'The', 'Muscat', 'search', 'engine', 'comes']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(to_be_stemmed)\n",
    "print(tokens[:100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['martin', 'porter', '-', 'wikipedia', 'martin', 'porter', 'from', 'wikipedia', ',', 'the', 'free', 'encyclopedia', 'jump', 'to', 'navig', 'jump', 'to', 'search', 'for', 'the', 'musician', ',', 'see', 'martin', 'porter', '(', 'musician', ')', '.', 'martin', 'F.', 'porter', 'is', 'the', 'inventor', 'of', 'the', 'porter', 'stemmer', ',', '[', '1', ']', 'one', 'of', 'the', 'most', 'common', 'algorithm', 'for', 'stem', 'english', ',', '[', '2', ']', '[', '3', ']', 'and', 'the', 'snowbal', 'program', 'framework', '.', 'hi', '1980', 'paper', '``', 'An', 'algorithm', 'for', 'suffix', 'strip', \"''\", ',', 'propos', 'the', 'stem', 'algorithm', ',', 'ha', 'been', 'cite', 'over', '8000', 'time', '(', 'googl', 'scholar', ')', '.', '[', '4', ']', 'the', 'muscat', 'search', 'engin', 'come']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens[:100]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['martin', 'port', '-', 'wikiped', 'martin', 'port', 'from', 'wikiped', ',', 'the', 'fre', 'encycloped', 'jump', 'to', 'navig', 'jump', 'to', 'search', 'for', 'the', 'mus', ',', 'see', 'martin', 'port', '(', 'mus', ')', '.', 'martin', 'f.', 'port', 'is', 'the', 'inv', 'of', 'the', 'port', 'stem', ',', '[', '1', ']', 'on', 'of', 'the', 'most', 'common', 'algorithm', 'for', 'stem', 'engl', ',', '[', '2', ']', '[', '3', ']', 'and', 'the', 'snowbal', 'program', 'framework', '.', 'his', '1980', 'pap', '``', 'an', 'algorithm', 'for', 'suffix', 'stripping', \"''\", ',', 'propos', 'the', 'stem', 'algorithm', ',', 'has', 'been', 'cit', 'ov', '8000', 'tim', '(', 'googl', 'scholar', ')', '.', '[', '4', ']', 'the', 'musc', 'search', 'engin', 'com']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens[:100]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-31\n",
    "# Define the variable saying to contain the list ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is',\n",
    "# 'said', 'than', 'done', '.']. Process the list using a for loop, and store the result in a new list lengths. Hint:\n",
    "# begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to\n",
    "# add another length value to the list.\n",
    "\n",
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']\n",
    "lengths = []\n",
    "\n",
    "for s in saying:\n",
    "    lengths.append(len(s))\n",
    "\n",
    "lengths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-32\n",
    "# Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating way'.\n",
    "# (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky’s\n",
    "# famous nonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now write code to perform\n",
    "# the following tasks:\n",
    "# a: Split silly into a list of strings, one per word, using Python’s split() operation, and save this to a variable\n",
    "# called bland.\n",
    "\n",
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "\n",
    "bland = silly.split()\n",
    "print(bland)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "'eoldrnnnna'"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b: Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.\n",
    "\n",
    "''.join([b[1] for b in bland])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "'newly formed bland ideas are inexpressible in an infuriating way'"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c: Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string\n",
    "# are separated with whitespace.\n",
    "\n",
    "' '.join(bland)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "# d: Print the words of silly in alphabetical order, one per line.\n",
    "\n",
    "for s in sorted(bland):\n",
    "    print(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-33\n",
    "# The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us\n",
    "# the index of the first position of the letter e.\n",
    "# a: What happens when you look up a substring, e.g., 'inexpressible'.index('re')?\n",
    "\n",
    "'inexpressible'.index('re')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b: Define a variable words containing a list of words. Now use words.index() to look up the position of an\n",
    "# individual word.\n",
    "\n",
    "words = [\"I'm\", 'too', 'tired', 'think', 'of', 'a', 'more',\n",
    "         'original', 'list']\n",
    "words.index('tired')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c: Define a variable silly as in Exercise 32. Use the index() function in combination with list slicing to build a\n",
    "# list phrase consisting of all the words up to (but not including) in in silly.\n",
    "\n",
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "\n",
    "phrase = [i for i in silly.split()[:silly.split().index('in')]]\n",
    "phrase"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# Exercise 3-34:\n",
    "# Write code to convert nationality adjectives such as Canadian and Australian to their corresponding nouns Canada\n",
    "# and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names).\n",
    "\n",
    "import os\n",
    "path = '/home/af/Dokumenter/Programs/NaturalLanguageProcessingwithPython-py39/Chapter 3/'\n",
    "os.chdir(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('Nationalities.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    nats = list(reader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Abkhazia', 'Abkhaz,?Abkhazian'],\n ['Afghanistan', 'Afghan'],\n ['?land Islands', '?land Island'],\n ['Albania', 'Albanian'],\n ['Algeria', 'Algerian']]"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nats[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "nats_edited = [[unicodedata.normalize(\"NFKD\", item) for item in pair] for pair in nats]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Abkhazia', 'Abkhaz,?Abkhazian'],\n ['Afghanistan', 'Afghan'],\n ['?land Islands', '?land Island'],\n ['Albania', 'Albanian'],\n ['Algeria', 'Algerian']]"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nats_edited[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def return_country_name(nationality, l):\n",
    "    \"\"\"\n",
    "    Returns the country name for a given nationality adjective.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    nationality: string with nationality adjective\n",
    "    l:           list of sublists. Country name is the first item in the\n",
    "                 sublist, nationality adjective the second.  Original list\n",
    "                 was modified from the one available at\n",
    "                 https://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_for_countries_and_nations\n",
    "    \"\"\"\n",
    "    for item in l:\n",
    "        if nationality in item[1]:\n",
    "            return item[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "'Sweden'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_country_name(\"Swedish\", nats_edited)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-35\n",
    "# Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun.\n",
    "# Investigate this phenomenon with the help of a corpus and the findall() method for searching tokenized text\n",
    "# described in Useful Applications of Regular Expressions. The post is at\n",
    "# http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html.\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "bw = nltk.Text(brown.words())\n",
    "bw.findall(r\"<as> <best> <as> <.*> <can>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bw.findall(r\"<as> <best> <.*> <can>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best they can\n"
     ]
    }
   ],
   "source": [
    "bw.findall(r\"<best> <.*> <can>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt:\n",
      "\t\n",
      "austen-persuasion.txt:\n",
      "\t\n",
      "austen-sense.txt:\n",
      "\t\n",
      "bible-kjv.txt:\n",
      "\t\n",
      "blake-poems.txt:\n",
      "\t\n",
      "bryant-stories.txt:\n",
      "\t\n",
      "burgess-busterbrown.txt:\n",
      "\t\n",
      "carroll-alice.txt:\n",
      "\t\n",
      "chesterton-ball.txt:\n",
      "\t\n",
      "chesterton-brown.txt:\n",
      "\t\n",
      "chesterton-thursday.txt:\n",
      "\t\n",
      "edgeworth-parents.txt:\n",
      "\t\n",
      "melville-moby_dick.txt:\n",
      "\tbest we can\n",
      "milton-paradise.txt:\n",
      "\t\n",
      "shakespeare-caesar.txt:\n",
      "\t\n",
      "shakespeare-hamlet.txt:\n",
      "\t\n",
      "shakespeare-macbeth.txt:\n",
      "\t\n",
      "whitman-leaves.txt:\n",
      "\tbest I can\n"
     ]
    }
   ],
   "source": [
    "for text in nltk.corpus.gutenberg.fileids():\n",
    "    print(text + \":\")\n",
    "    print(\"\\t\", end = '')\n",
    "    nltk.Text(nltk.corpus.gutenberg.words(text)).findall(r\"<best> <.*> <can>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "ps = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\"]\n",
    "phrases = [\"as best as X can\", \"as best X can\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'hits' from 'googlesearch' (/home/af/Dokumenter/Programs/miniconda3/envs/NaturalLanguageProcessingwithPython-py39/lib/python3.9/site-packages/googlesearch/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-99-815a0a13d1e8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mgooglesearch\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mhits\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mph\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mphrases\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'hits' from 'googlesearch' (/home/af/Dokumenter/Programs/miniconda3/envs/NaturalLanguageProcessingwithPython-py39/lib/python3.9/site-packages/googlesearch/__init__.py)"
     ]
    }
   ],
   "source": [
    "from googlesearch import hits\n",
    "\n",
    "for p in ps:\n",
    "    for ph in phrases:\n",
    "\n",
    "        check = re.sub('X', p, ph)\n",
    "        no_results = hits(check)\n",
    "\n",
    "        print(\"Number of hits for \\\"{}\\\": {} {:,}\".format(check, \" \" * (19 - len(check)), no_results))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "def compare_best_as(pronoun):\n",
    "\n",
    "    return IFrame(\"https://books.google.com/ngrams/graph?content=as+best+as+\" + pronoun + \"+can%2C+as+best+\" + pronoun +  \"+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\",\n",
    "       width=1100, height=700 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x7f2ac2308a60>",
      "text/html": "\n        <iframe\n            width=\"1100\"\n            height=\"700\"\n            src=\"https://books.google.com/ngrams/graph?content=as+best+as+they+can%2C+as+best+they+can&year_start=1800&year_end=2000&corpus=15&smoothing=3\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        "
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_best_as(\"they\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oh', 'hai', '.', 'In', 'teh', 'beginnin', 'Ceiling', 'Cat', 'maded', 'teh', 'skiez', 'An', 'da', 'Urfs', ',', 'but', 'he', 'did', 'not', 'eated']\n"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-36\n",
    "# Study the lolcat version of the book of Genesis, accessible as nltk.corpus.genesis.words('lolcat.txt'), and the\n",
    "# rules for converting text into lolspeak at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define\n",
    "# regular expressions to convert English words into corresponding lolspeak words.\n",
    "\n",
    "lolcat_gen = nltk.corpus.genesis.words('lolcat.txt')\n",
    "print(lolcat_gen[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('tranzlator.yml') as f:\n",
    "\n",
    "    d = yaml.load(f, Loader = yaml.FullLoader)\n",
    "\n",
    "\n",
    "def convert_to_lolspeak(text):\n",
    "    \"\"\"\n",
    "    Converts a text to lolspeak.\n",
    "    \"\"\"\n",
    "\n",
    "    text_list = text.split()\n",
    "\n",
    "    # convert everything to lowercase so we can search for terms\n",
    "    # in the dictionary\n",
    "    text_list = [t.lower() for t in text_list]\n",
    "\n",
    "    # replace those terms that can be found in the lolspeak dictionary\n",
    "    for i in range(len(text_list)):\n",
    "        if text_list[i] in d:\n",
    "            text_list[i] = d[text_list[i]].upper()\n",
    "\n",
    "    new_list = []\n",
    "\n",
    "    # use regular expressions for the rest\n",
    "    for t in text_list:\n",
    "        # the values with matches in the lolspeak dictionary\n",
    "        # have been convered to uppercase words,\n",
    "        # so the next conditional will run only\n",
    "        # on words that haven't been modified\n",
    "        if t.islower():\n",
    "\n",
    "            org = [r'er', r'ight', r'[^s]s\\b', r'ss', r'tion|sion', r'ing',\n",
    "                   r'ture', r'ove', r'ear', r'ee']\n",
    "            sub = ['r', 'ite', 'z', 's', 'shun', 'in', 'chur', 'oov', 'er', 'e']\n",
    "\n",
    "            for i in range(len(org)):\n",
    "                t = re.sub(org[i], sub[i], t)\n",
    "\n",
    "            if len(t) > 3 and t[-1] != 'd':\n",
    "                t = re.sub('ed', 'd', t)\n",
    "\n",
    "        # for stylistic reasons, everything will be returned in uppercase\n",
    "        new_list.append(t.upper())\n",
    "\n",
    "    # there's just one grammatical exception I'm going to deal with\n",
    "    # in this function: converting \"can I\" to \"I CAN\"\n",
    "    string = ' '.join(new_list)\n",
    "    string = re.sub(r'CAN I', 'I CAN', string)\n",
    "\n",
    "    return string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I CAN TEH CHEEZBURGER ?\n"
     ]
    }
   ],
   "source": [
    "test = \"can i the cheeseburger ?\"\n",
    "\n",
    "print(convert_to_lolspeak(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "# Exercise: 3-37\n",
    "# Read about the re.sub() function for string substitution using regular expressions, using help(re.sub) and by\n",
    "# consulting the further readings for this chapter. Use re.sub in writing code to remove HTML tags from an HTML file,\n",
    "# and to normalize whitespace.\n",
    "\n",
    "example = \"\"\"\n",
    "<HTML>\n",
    "\n",
    "<HEAD>\n",
    "\n",
    "<TITLE>Your Title Here</TITLE>\n",
    "\n",
    "</HEAD>\n",
    "\n",
    "<BODY BGCOLOR=\"FFFFFF\">\n",
    "\n",
    "<CENTER><IMG SRC=\"clouds.jpg\" ALIGN=\"BOTTOM\"> </CENTER>\n",
    "\n",
    "<HR>\n",
    "\n",
    "<a href=\"http://somegreatsite.com\">Link Name</a>\n",
    "\n",
    "is a link to another nifty site\n",
    "\n",
    "<H1>This is a Header</H1>\n",
    "\n",
    "<H2>This is a Medium Header</H2>\n",
    "\n",
    "Send me mail at <a href=\"mailto:support@yourcompany.com\">\n",
    "\n",
    "support@yourcompany.com</a>.\n",
    "\n",
    "<P> This is a new paragraph!\n",
    "\n",
    "<P> <B>This is a new paragraph!</B>\n",
    "\n",
    "<BR> <B><I>This is a new sentence without a paragraph break, in bold italics.</I></B>\n",
    "\n",
    "<HR>\n",
    "\n",
    "</BODY>\n",
    "\n",
    "</HTML>\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "' Your Title Here Link Name is a link to another nifty site This is a Header This is a Medium Header Send me mail at support@yourcompany.com. This is a new paragraph! This is a new paragraph! This is a new sentence without a paragraph break, in bold italics. '"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting rid of tages\n",
    "example = re.sub(r'<.*?>', '', example)\n",
    "\n",
    "# normalizing whitespace\n",
    "example = re.sub(r'\\s+', ' ', example)\n",
    "example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "['long-\\nterm', 'encyclo-\\npedia']"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-38\n",
    "# An interesting challenge for tokenization is words that have been split across a linebreak. E.g., if long-term is\n",
    "# split, then we have the string long-\\nterm.\n",
    "# a: Write a regular expression that identifies words that are hyphenated at a line-break. The expression will\n",
    "# need to include the \\n character.\n",
    "\n",
    "test = \"I'm trying to find the long-\\nterm solution in this encyclo-\\npedia.\"\n",
    "re.findall(r'\\b\\w*-\\n\\w*\\b', test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "\"I'm trying to find the long-term solution in this encyclo-pedia.\""
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b: Use re.sub() to remove the \\n character from these words.\n",
    "\n",
    "re.sub(r'-\\n', '-', test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jean-christophe\n",
      "jean-pierre\n"
     ]
    }
   ],
   "source": [
    "# c: How might you identify words that should not remain hyphenated once the newline is removed, e.g.,\n",
    "# 'encyclo-\\npedia'?\n",
    "\n",
    "for w in wordlist:\n",
    "    if '-' in w:\n",
    "        print(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "brown_words = sorted(set([w.lower() for w in nltk.corpus.brown.words()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def check_line_break(text, wordlist):\n",
    "    \"\"\"\n",
    "    Finds hyphenated words with line breaks in a text and\n",
    "    removes hyphens if they are the results of a split on\n",
    "    a line break.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    text:     String which may have hyphens.\n",
    "    wordlist: List of accepted word forms.\n",
    "    \"\"\"\n",
    "\n",
    "    # the function will only run if there's a hyphen in the string.\n",
    "    if '-' in text:\n",
    "\n",
    "        # find all the hyphenated words\n",
    "        hyphen = re.findall(r'\\b\\w*-\\n\\w*\\b', text)\n",
    "\n",
    "        # perform lookup for each hyphenated word\n",
    "        for h in hyphen:\n",
    "            # remove the newspace to get the hyphenated form\n",
    "            check = re.sub(r'-\\n', '-', h)\n",
    "\n",
    "            # keep the hyphen if the hyphenated form is in the wordlist\n",
    "            if check in wordlist:\n",
    "                text =  re.sub(h, check, text)\n",
    "            # otherwise remove it\n",
    "            else:\n",
    "                text = re.sub(h, re.sub(r'-\\n', '', h), text)\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "\"I'm trying to find the long-term solution in this encyclopedia.\""
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"I'm trying to find the long-\\nterm solution in this encyclo-\\npedia.\"\n",
    "\n",
    "check_line_break(test, brown_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "# Exercise: 3-39\n",
    "# Read the Wikipedia entry on Soundex. Implement this algorithm in Python.\n",
    "\n",
    "def get_soundex_digit(chr):\n",
    "    \"\"\"\n",
    "    Returns a numerical digit based on a consonant's value\n",
    "    in the Soundex algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    cs = ['bfpv', 'cgjkqsxz', 'dt', 'l', 'mn', 'r']\n",
    "    for i in range(len(cs)):\n",
    "        if chr.lower() in cs[i]:\n",
    "            return str(i + 1)\n",
    "\n",
    "    return \"0\"\n",
    "\n",
    "def remove_consecutive_duplicates(text):\n",
    "    \"\"\"\n",
    "    Returns a string with consecutive duplicates removed.\n",
    "    \"\"\"\n",
    "\n",
    "    new_text = text[0]\n",
    "\n",
    "    for i in text[1:]:\n",
    "        if i != new_text[-1]:\n",
    "            new_text += i\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def create_soundex(name):\n",
    "    \"\"\"\n",
    "    Uses the Soundex algorithm to create an index for a name.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start with the first letter.  Capitalize this letter\n",
    "    # if it's not already capitalized.\n",
    "    soundex = name[0].upper()\n",
    "\n",
    "    # Remove all occurrences of h and w\n",
    "    name = re.sub(r'[hw]', '', name[1:])\n",
    "\n",
    "    # Replace vowels with zeros. After we assign values to consonants,\n",
    "    # consecutive identical values will be removed, except in\n",
    "    # the case that they are on either side of a vowel. So the zeros will\n",
    "    # be retained until this point.  After this they will be deleted.\n",
    "    name = re.sub(r'[aeiouy]', '0', name)\n",
    "\n",
    "    # Replace all consonants with digits using get_soundex_digit\n",
    "    new_name = ''\n",
    "    for n in name:\n",
    "        new_name += (re.sub(r'[^0]', get_soundex_digit(n), n))\n",
    "\n",
    "    name = new_name\n",
    "\n",
    "    # Replace all adjacent identical digits with one digit\n",
    "    name = remove_consecutive_duplicates(name)\n",
    "\n",
    "    # First digit can't be of the same group as the beginning character\n",
    "    if get_soundex_digit(soundex) == name[0]:\n",
    "        name = name[1:]\n",
    "\n",
    "    # Remove zeros\n",
    "    name = re.sub(r'0', '', name)\n",
    "\n",
    "    # In case everything after the first letter is a vowel\n",
    "    if len(name) == 0:\n",
    "        return soundex + ('000')\n",
    "\n",
    "    soundex += name\n",
    "\n",
    "    # Index can only be four characters long\n",
    "    if len(soundex) == 4:\n",
    "        return soundex\n",
    "    elif len(soundex) > 4:\n",
    "        return soundex[:4]\n",
    "    else:\n",
    "        # Add zeros if the index is shorter than 4\n",
    "        while len(soundex) < 4:\n",
    "            soundex += \"0\"\n",
    "\n",
    "        return soundex"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M000', 'M620', 'M652', 'M620', 'M560', 'M653', 'M610', 'M625']\n"
     ]
    }
   ],
   "source": [
    "names = [\"Mayo\", \"Meiers\", \"Mierins\", \"Miers\", \"Mimre\", \"Miranda\",\n",
    "         \"Mirfe\", \"Mirissimo\"]\n",
    "\n",
    "print([create_soundex(n) for n in names])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# Exercise: 3-40\n",
    "# Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier\n",
    "# exercise on reading difficulty. E.g., compare ABC Rural News and ABC Science News (nltk.corpus.abc). Use Punkt\n",
    "# to perform sentence segmentation.\n",
    "\n",
    "abc = nltk.corpus.abc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "def get_ari(corpus, cat):\n",
    "    \"\"\"\n",
    "    Returns the Automated Readability Index (ARI) of a given\n",
    "    category of a NLTK corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate total letters in the category\n",
    "    total_letters = 0\n",
    "    for w in corpus.words(fileids = cat):\n",
    "        total_letters += len(w)\n",
    "\n",
    "    # calculate average number of letters per word\n",
    "    mu_w = total_letters/len(corpus.words(fileids = cat))\n",
    "\n",
    "    # calculate average number of words per sentence\n",
    "    mu_s = len(corpus.words(fileids = cat)) / len(corpus.sents(fileids = cat))\n",
    "\n",
    "    return (4.71 * mu_w) + (0.5 * mu_s) - 21.43"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the ABC Corpus, the Automated Readability Index (ARI) for the category:\n",
      "\n",
      "                                             ...\"Rural\" is 12.3487.\n",
      "                                             ...\"Science\" is 12.5276.\n"
     ]
    }
   ],
   "source": [
    "print(\"In the ABC Corpus, the Automated Readability Index (ARI) for the category:\\n\")\n",
    "\n",
    "for f in abc.fileids():\n",
    "    ari = get_ari(abc, f)\n",
    "    f = re.sub('.txt', '', f)\n",
    "    print(\"{:45}...\\\"{}\\\" is {:.4f}.\".format(\"\",f.title(), ari))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: 3-41\n",
    "# Rewrite the following nested loop as a nested list comprehension:\n",
    "# >>> words = ['attribution', 'confabulation', 'elocution',\n",
    "# ...          'sequoia', 'tenacious', 'unidirectional']\n",
    "# >>> vsequences = set()\n",
    "# >>> for word in words:\n",
    "# ...     vowels = []\n",
    "# ...     for char in word:\n",
    "# ...         if char in 'aeiou':\n",
    "# ...             vowels.append(char)\n",
    "# ...     vsequences.add(''.join(vowels))\n",
    "# >>> sorted(vsequences)\n",
    "# ['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
    "\n",
    "words = ['attribution', 'confabulation', 'elocution',\n",
    "         'sequoia', 'tenacious', 'unidirectional']\n",
    "\n",
    "sorted(set([''.join([char for char in word if char in 'aeiou']) for word in words]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# Exercise: 3-42\n",
    "# Use WordNet to create a semantic index for a text collection. Extend the concordance search program in Example 3-1,\n",
    "# indexing each word using the offset of its first synset, e.g., wn.synsets('dog')[0].offset (and optionally the\n",
    "# offset of some of its ancestors in the hypernym hierarchy).\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def make_semantic_index(text, ancestors = False):\n",
    "    \"\"\"\n",
    "    Suffixes the offset of a word's first WordNet synset to\n",
    "    lemmatized words in a text.  Only words that are not\n",
    "    stopwords are analyzed.\n",
    "    \"\"\"\n",
    "\n",
    "    # get stopwords\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    # use lemmas, so all word forms return the same offset\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "    indexed = []\n",
    "\n",
    "    for t in text.split():\n",
    "\n",
    "        t_lemma = wnl.lemmatize(t.lower())\n",
    "\n",
    "        # only add offsets to non-stopwords\n",
    "        if t_lemma not in stop:\n",
    "\n",
    "            # some words have no synset, and looking\n",
    "            # them up will throw an error\n",
    "            try:\n",
    "                index = str(wn.synsets(t_lemma)[0].offset())\n",
    "                index += wn.synsets(t_lemma)[0].pos()\n",
    "\n",
    "                if ancestors:\n",
    "\n",
    "                    hp = str(wn.synsets(t_lemma)[0].hypernyms()[0].offset())\n",
    "                    hp += wn.synsets(t_lemma)[0].hypernyms()[0].pos()\n",
    "                    index = '||'.join((index, hp))\n",
    "\n",
    "                indexed.append('|'.join((t, index)))\n",
    "            except IndexError:\n",
    "                indexed.append(t)\n",
    "\n",
    "        else:\n",
    "            indexed.append(t)\n",
    "\n",
    "    # join the list as a string and return it\n",
    "    return ' '.join(indexed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/2019/10/21/us/california-homeless-backlash.html?fallback=0&recId=1SYcSgtQJYxBu1CiMNgqELRgdmq&locked=0&geoContinent=NA&geoRegion=WA&recAlloc=top_conversion&geoCountry=US&blockId=most-popular&imp_id=189862191&action=click&module=trending&pgtype=Article&region=Footer'\n",
    "homeless = return_URL_contents(url)\n",
    "\n",
    "# getting rid of header and footer - used trial and error to find this\n",
    "homeless = homeless[270:-2434]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "'meless-backlash.html Advertisement|7248801n Continue|2684924v reading|5808794n the main|9345932n story|7221094n Supported|2556126v by Continue|2684924v reading|5808794n the main|9345932n story|7221094n As Homelessness|13943053n Surges|7440240n in California, So Does|8132955n a Backlash|7350192n Tent|4411264n encampments|8518171n across|272951r California|9060768n are testing|639975n residents’ tolerance|5033410n and compassion|7553741n as street|4334599n conditions|13920835n deteriorate. A homeless|10182190n encampment|8518171n in San Francisco. Homelessness|13943053n is an expanding|2077148v crisis|13933560n that many|1551633a California|9060768n residents|10523519n say|14485526n has|13888783n tested|2531625v the tolerance|5033410n and liberal|10256756n values|5856388n for which the state|8654360n is better|5143558n known. Credit... Jim Wilson/The New|1640850a York|8159924n Times|7309599n By Thomas|11339534n Fuller|10985653n , Tim Arango and Louis|11141709n Keene Oct. 21, 2019 [Sign u'"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_semantic_index(homeless)[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "'meless-backlash.html Advertisement|7248801n||7247071n Continue|2684924v||2367363v reading|5808794n||5808557n the main|9345932n||9225146n story|7221094n||6598915n Supported by Continue|2684924v||2367363v reading|5808794n||5808557n the main|9345932n||9225146n story|7221094n||6598915n As Homelessness|13943053n||13920835n Surges|7440240n||7405893n in California, So Does|8132955n||8123167n a Backlash|7350192n||7309781n Tent|4411264n||4191595n encampments|8518171n||8651247n across California are testing|639975n||639556n residents’ tolerance|5033410n||5032565n and compassion|7553741n||7553301n as street|4334599n||4426618n conditions|13920835n||24720n deteriorate. A homeless|10182190n||9630641n encampment|8518171n||8651247n in San Francisco. Homelessness|13943053n||13920835n is an expanding|2077148v||230746v crisis|13933560n||14411243n that many California residents|10523519n||9620078n say|14485526n||14483917n has|13888783n||13888491n tested|2531625v||670261v the tolerance|5033410n||5032565n a'"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_semantic_index(homeless, ancestors = True)[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "Synset('homelessness.n.01')"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset_from_pos_and_offset('n', 13943053)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['13943053n', '13920835n', '13920835n', '10182190n', '13943053n', '13920835n', '13933560n', '14411243n', '10523519n', '14485526n', '14483917n', '13888783n', '13888491n']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'\\d{8}\\w', make_semantic_index(homeless, ancestors = True)[:1000]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('homelessness.n.01')\n",
      "Synset('condition.n.01')\n",
      "Synset('condition.n.01')\n",
      "Synset('homeless.n.01')\n",
      "Synset('homelessness.n.01')\n",
      "Synset('condition.n.01')\n",
      "Synset('crisis.n.01')\n",
      "Synset('situation.n.03')\n",
      "Synset('resident.n.01')\n",
      "Synset('say.n.01')\n",
      "Synset('opportunity.n.01')\n",
      "Synset('hour_angle.n.02')\n",
      "Synset('angular_distance.n.01')\n"
     ]
    }
   ],
   "source": [
    "for r in re.findall(r'\\d{8}\\w', make_semantic_index(homeless, ancestors = True)[:1000]):\n",
    "    print(wn.synset_from_pos_and_offset(r[8], int(r[:8])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "# Exercise: 3-43\n",
    "# With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr),\n",
    "# along with NLTK’s frequency distribution and rank correlation functionality (nltk.FreqDist,\n",
    "# nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity,\n",
    "# work with a single character encoding and just a few languages.\n",
    "\n",
    "import nltk, re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import udhr\n",
    "from nltk.metrics.spearman import *\n",
    "\n",
    "def guess_language(text, encoding = '-Latin1'):\n",
    "\n",
    "    # text has to be a list, so convert if it's not\n",
    "    if type(text) != list:\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "    # create frequency distribution for the letters in the text\n",
    "    fd = nltk.FreqDist([ch for ch in [ch for word in text for ch in word.lower() if word.isalpha()]])\n",
    "\n",
    "    # create a ranking of the most common letters in the text\n",
    "    ranked_letters = [l for r, l in sorted([(v, k) for k, v in fd.items()], reverse = True)]\n",
    "\n",
    "    # get a list of languages with specified encoding\n",
    "    languages = [l for l in nltk.corpus.udhr.fileids() if l.endswith(encoding)]\n",
    "\n",
    "    max_corr = float('-inf')\n",
    "    top_cand = ''\n",
    "\n",
    "    # loop through all the languages and find the one with the highest spearman correlation\n",
    "    for lang in languages:\n",
    "        fd = nltk.FreqDist([ch for ch in [ch for word in udhr.words(lang) for ch in word.lower() if word.isalpha()]])\n",
    "        fd_letters = [l for r, l in sorted([(v, k) for k, v in fd.items()], reverse = True)]\n",
    "        sc = spearman_correlation(ranks_from_sequence(ranked_letters),\n",
    "                               ranks_from_sequence(fd_letters))\n",
    "        if sc > max_corr:\n",
    "            top_cand, max_corr =  lang, sc\n",
    "\n",
    "    # replace underscore with hyphen\n",
    "    top_cand = re.sub('_', '-', top_cand)\n",
    "\n",
    "    # remove encoding from language name\n",
    "    top_cand = re.sub(encoding, '', top_cand)\n",
    "\n",
    "    print(\"The most likely language is \" + top_cand + \".\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "url = 'https://www.tagesschau.de/ausland/brexit-1083.html'\n",
    "\n",
    "de = return_URL_contents(url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely language is German-Deutsch.\n"
     ]
    }
   ],
   "source": [
    "guess_language(de)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "# Exercise: 3-44\n",
    "# Write a program that processes a text and discovers cases where a word has been used with a novel sense. For each\n",
    "# word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context.\n",
    "# (Note that this is a crude approach; doing it well is a difficult, open research problem.)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "\n",
    "def find_novel_usage(text, novelty_score = 0.1):\n",
    "    \"\"\"\n",
    "    Calculates the path similarity of adjoining words in a text\n",
    "    and prints out the words whose path similarity falls below\n",
    "    a certain threshold\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    text:          The text to be analyzed.  Must be a string.\n",
    "    novelty_score: The threshold below which pairs of words are\n",
    "                   considered to be used together in a novel way.\n",
    "                   Default is 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    # remove stopwords and puncutation\n",
    "    content = [t for t in test.split() if t.lower() not in stopwords.words('english')]\n",
    "    content = [re.sub(r'[.,!?\\'\"()\\[\\]]', '', c) for c in content]\n",
    "\n",
    "\n",
    "    connections = ''\n",
    "    path_scores = []\n",
    "\n",
    "    # loop through the pairs of words and find the highest similarity score\n",
    "    for i in range(len(content) - 1):\n",
    "        term1 = wn.synsets(content[i])\n",
    "        term2 = wn.synsets(content[i + 1])\n",
    "        score = [v for v in [t1.path_similarity(t2) for t1 in term1 for t2 in term2] if isinstance(v, float)]\n",
    "\n",
    "        # if there's no path similarity score, add zero\n",
    "        if len(score) == 0:\n",
    "            sim = 0\n",
    "        # otherwise, add the maximum value\n",
    "        else:\n",
    "            sim = max(score)\n",
    "\n",
    "        connections += ' | '.join((content[i], str(sim))) + \" | \"\n",
    "        path_scores.append(sim)\n",
    "\n",
    "    connections += content[-1]\n",
    "\n",
    "    print(\"Similarity scores of content words:\\n\")\n",
    "    print(connections, \"\\n\")\n",
    "\n",
    "    for i in range(len(path_scores)):\n",
    "        if path_scores[i] < novelty_score and path_scores[i] != 0:\n",
    "            print(\"Possible novel usage:\", content[i], \"&\", content[i + 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores of content words:\n",
      "\n",
      "saw | 0.09090909090909091 | airplane | 0.1 | window | 0.16666666666666666 | apartment | 0.1111111111111111 | country \n",
      "\n",
      "Possible novel usage: saw & airplane\n"
     ]
    }
   ],
   "source": [
    "test = 'i saw an airplane from the window of my apartment in the country'\n",
    "\n",
    "find_novel_usage(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('proverb.n.01'),\n Synset('saw.n.02'),\n Synset('power_saw.n.01'),\n Synset('saw.v.01'),\n Synset('see.v.01'),\n Synset('understand.v.02'),\n Synset('witness.v.02'),\n Synset('visualize.v.01'),\n Synset('see.v.05'),\n Synset('learn.v.02'),\n Synset('watch.v.03'),\n Synset('meet.v.01'),\n Synset('determine.v.08'),\n Synset('see.v.10'),\n Synset('see.v.11'),\n Synset('see.v.12'),\n Synset('visit.v.01'),\n Synset('attend.v.02'),\n Synset('see.v.15'),\n Synset('go_steady.v.01'),\n Synset('see.v.17'),\n Synset('see.v.18'),\n Synset('see.v.19'),\n Synset('examine.v.02'),\n Synset('experience.v.01'),\n Synset('see.v.22'),\n Synset('see.v.23'),\n Synset('interpret.v.01')]"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('saw')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0625"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saw = wn.synset('proverb.n.01')\n",
    "saw.path_similarity(wn.synset('saw.n.02'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.semi_supervised.label_propagation'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-135-5937d24b8d48>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# text normalization.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"On\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"the\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"28\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Apr.\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"2010\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\",\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Dr.\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Banks\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"bought\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"a\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"chair\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"for\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"£35\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\".\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/NaturalLanguageProcessingwithPython-py39/lib/python3.9/site-packages/normalise/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalisation\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist_NSWs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrejoin\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenize_basic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/NaturalLanguageProcessingwithPython-py39/lib/python3.9/site-packages/normalise/normalisation.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagger\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtagify\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_digbased\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplitter\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretagify\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclass_ALPHA\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrun_clfALPHA\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclass_NUMB\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrun_clfNUMB\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgen_frame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtag_MISC\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtag_MISC\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/NaturalLanguageProcessingwithPython-py39/lib/python3.9/site-packages/normalise/class_ALPHA.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetect\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmod_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtagger\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtagify\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_digbased\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0macr_pattern\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclass_NUMB\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mgen_frame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplitter\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretagify\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnormalise\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmeasurements\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmeas_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmeas_dict_pl\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/NaturalLanguageProcessingwithPython-py39/lib/python3.9/site-packages/normalise/class_NUMB.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'{}/data/clf_NUMB.pickle'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmod_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m     \u001B[0mclf_NUMB\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"__main__\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sklearn.semi_supervised.label_propagation'"
     ]
    }
   ],
   "source": [
    "# Exercise: 3-45\n",
    "# Read the article on normalization of non-standard words (Sproat et al., 2001), and implement a similar system for\n",
    "# text normalization.\n",
    "\n",
    "from normalise import *\n",
    "\n",
    "text = [\"On\", \"the\", \"28\", \"Apr.\", \"2010\", \",\", \"Dr.\", \"Banks\", \"bought\", \"a\", \"chair\", \"for\", \"£35\", \".\"]\n",
    "print(normalise(text, verbose = False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}